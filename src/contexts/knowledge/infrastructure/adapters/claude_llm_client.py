"""
Anthropic Claude LLM Client Adapter

Implements ILLMClient for Anthropic's Claude API.
Provides text generation capabilities for query rewriting and other knowledge services.

Constitution Compliance:
- Article II (Hexagonal): Infrastructure adapter implementing application port
- Article V (SOLID): SRP - Claude API interaction only

Warzone 4: AI Brain - BRAIN-024A, BRAIN-024B
"""

from __future__ import annotations

import json
import os
from typing import TYPE_CHECKING, AsyncIterator

import httpx
import structlog

from ...application.ports.i_llm_client import LLMRequest, LLMResponse, LLMError

if TYPE_CHECKING:
    pass

logger = structlog.get_logger()


# Default model configuration
DEFAULT_MODEL = "claude-3-5-sonnet-20241022"
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 1000

# Supported Claude models
CLAUDE_MODELS = {
    "claude-3-5-sonnet-20241022": "Claude 3.5 Sonnet",
    "claude-3-5-haiku-20241022": "Claude 3.5 Haiku",
    "claude-3-opus-20240229": "Claude 3 Opus",
    "claude-3-sonnet-20240229": "Claude 3 Sonnet",
    "claude-3-haiku-20240307": "Claude 3 Haiku",
}


class ClaudeLLMClient:
    """
    Anthropic Claude API client for LLM text generation.

    Implements the ILLMClient protocol using Anthropic's Claude API.
    Designed for efficient text generation in knowledge services like
    query rewriting and entity extraction.

    Claude uses a different message format than OpenAI:
    - System prompt is a top-level field (not in messages array)
    - Messages array with role/content structure
    - Content is returned as blocks with type "text"
    - Token counting is done via usage field (input_tokens, output_tokens)

    Configuration via environment variables:
        - ANTHROPIC_API_KEY: Required API authentication key
        - ANTHROPIC_MODEL: Model name (default: claude-3-5-sonnet-20241022)
        - ANTHROPIC_BASE_URL: Custom API base URL (for testing/proxies)

    Attributes:
        _model: Claude model identifier
        _api_key: Anthropic API authentication key
        _base_url: Claude API endpoint URL
        _timeout: HTTP request timeout in seconds

    Example (non-streaming):
        >>> client = ClaudeLLMClient(model="claude-3-5-sonnet-20241022")
        >>> request = LLMRequest(
        ...     system_prompt="Rewrite queries for better search.",
        ...     user_prompt="protagonist motivation"
        ... )
        >>> response = await client.generate(request)
        >>> print(response.text)

    Example (streaming):
        >>> async for chunk in client.generate_stream(request):
        ...     print(chunk, end="", flush=True)
    """

    def __init__(
        self,
        model: str | None = None,
        api_key: str | None = None,
        base_url: str | None = None,
        timeout: int = 60,
    ):
        """
        Initialize the Claude LLM client.

        Args:
            model: Claude model name (defaults to ANTHROPIC_MODEL env var or claude-3-5-sonnet-20241022)
            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)
            base_url: Custom API base URL (for testing/proxies)
            timeout: HTTP request timeout in seconds (default: 60)

        Raises:
            ValueError: If API key is not provided and not in environment
        """
        self._model = model or os.getenv("ANTHROPIC_MODEL", DEFAULT_MODEL)
        self._api_key = api_key or os.getenv("ANTHROPIC_API_KEY", "")
        self._timeout = timeout

        if not self._api_key:
            raise ValueError(
                "ANTHROPIC_API_KEY environment variable or api_key parameter is required"
            )
        # After validation, _api_key is guaranteed to be non-empty str

        self._base_url = base_url or os.getenv(
            "ANTHROPIC_BASE_URL", "https://api.anthropic.com/v1/messages"
        )

    async def generate_stream(
        self, request: LLMRequest
    ) -> AsyncIterator[str]:
        """
        Generate streaming text using the Claude API.

        Yields text chunks as they are generated by Claude.
        This provides a better user experience for long generations.

        Claude streaming format:
        - Server-Sent Events (SSE) with "data:" prefix
        - Each event is a JSON object with "type" field
        - Types: "message_start", "content_block_delta", "message_delta", "message_stop"
        - Text deltas are in "delta" -> "text" field

        Args:
            request: LLMRequest with system/user prompts and parameters

        Yields:
            Text chunks as they are generated

        Raises:
            LLMError: If API call fails

        Example:
            >>> async for chunk in client.generate_stream(
            ...     LLMRequest(system_prompt="You are helpful.", user_prompt="Tell me a story")
            ... ):
            ...     print(chunk, end="", flush=True)
        """
        log = logger.bind(model=self._model, temperature=request.temperature)

        request_body = self._build_request_body(request)
        request_body["stream"] = True  # Enable streaming mode

        log.debug(
            "claude_stream_start",
            prompt_length=len(request.user_prompt),
            has_system_prompt=bool(request.system_prompt),
        )

        headers: dict[str, str] = {
            "Content-Type": "application/json",
            "x-api-key": self._api_key,  # type: ignore[dict-item]
            "anthropic-version": "2023-06-01",
        }

        try:
            async with httpx.AsyncClient(timeout=self._timeout) as client:
                async with client.stream(
                    "POST",
                    str(self._base_url),
                    headers=headers,
                    json=request_body,
                ) as response:
                    response.raise_for_status()

                    async for line in response.aiter_lines():
                        if not line.startswith("data: "):
                            continue

                        data_str = line[6:].strip()  # Remove "data: " prefix

                        # Skip empty lines and event types
                        if not data_str or data_str == "[DONE]":
                            continue

                        try:
                            event = json.loads(data_str)
                            event_type = event.get("type")

                            # Extract text from content_block_delta events
                            if event_type == "content_block_delta":
                                delta = event.get("delta", {})
                                if "text" in delta:
                                    yield delta["text"]

                            # Log final usage from message_stop event
                            elif event_type == "message_stop":
                                log.debug("claude_stream_complete")

                        except (json.JSONDecodeError, KeyError) as e:
                            log.warning("claude_stream_parse_error", error=str(e))
                            continue

        except httpx.HTTPStatusError as e:
            log.error(
                "claude_stream_http_error",
                status_code=e.response.status_code,
                error=str(e),
            )
            error_message = self._format_http_error(e)
            raise LLMError(error_message) from e
        except httpx.RequestError as e:
            log.error("claude_stream_request_failed", error=str(e))
            raise LLMError(f"Claude API stream request failed: {e}") from e

    async def generate(self, request: LLMRequest) -> LLMResponse:
        """
        Generate text using the Claude API.

        Claude API format:
        - System prompt is a separate top-level field
        - User prompt goes in messages array with role="user"
        - Response comes as content blocks with type="text"

        Args:
            request: LLMRequest with system/user prompts and parameters

        Returns:
            LLMResponse with generated text and metadata

        Raises:
            LLMError: If API call fails

        Example:
            >>> response = await client.generate(
            ...     LLMRequest(
            ...         system_prompt="You are a search expert.",
            ...         user_prompt="Rewrite: brave warrior",
            ...         temperature=0.5
            ...     )
            ... )
            >>> print(response.text)
        """
        log = logger.bind(model=self._model, temperature=request.temperature)

        # Build request body for Claude API
        request_body = self._build_request_body(request)

        log.debug(
            "claude_generate_start",
            prompt_length=len(request.user_prompt),
            has_system_prompt=bool(request.system_prompt),
        )

        try:
            response = await self._make_request(request_body)
            text, usage = self._extract_response_text(response)

            log.debug(
                "claude_generate_complete",
                response_length=len(text),
                input_tokens=usage.get("input_tokens") if usage else None,
                output_tokens=usage.get("output_tokens") if usage else None,
            )

            tokens_used = None
            input_tokens = None
            output_tokens = None

            if usage:
                input_tokens = usage.get("input_tokens", 0)
                output_tokens = usage.get("output_tokens", 0)
                tokens_used = (input_tokens + output_tokens) or None

            return LLMResponse(
                text=text,
                model=str(self._model),
                tokens_used=tokens_used,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                raw_usage=usage,
            )

        except httpx.HTTPStatusError as e:
            log.error(
                "claude_http_error",
                status_code=e.response.status_code,
                error=str(e),
            )
            error_message = self._format_http_error(e)
            raise LLMError(error_message) from e
        except httpx.RequestError as e:
            log.error("claude_request_failed", error=str(e))
            raise LLMError(f"Claude API request failed: {e}") from e
        except (KeyError, IndexError, TypeError) as e:
            log.error("claude_response_parse_failed", error=str(e))
            raise LLMError(f"Failed to parse Claude response: {e}") from e

    def _build_request_body(self, request: LLMRequest) -> dict:
        """
        Build request body for Claude API.

        Claude format:
        {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": 1000,
            "temperature": 0.7,
            "system": "System instructions here",
            "messages": [{"role": "user", "content": "User prompt here"}]
        }

        Args:
            request: LLMRequest with system and user prompts

        Returns:
            Request body dict for Claude API
        """
        body = {
            "model": self._model,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "messages": [
                {
                    "role": "user",
                    "content": request.user_prompt,
                }
            ],
        }

        # Add system prompt if provided (Claude separates this from messages)
        if request.system_prompt:
            body["system"] = request.system_prompt

        return body

    async def _make_request(self, request_body: dict) -> dict:
        """
        Make HTTP request to Claude API.

        Args:
            request_body: Request payload for the API

        Returns:
            Parsed JSON response

        Raises:
            httpx.HTTPStatusError: On HTTP errors (4xx, 5xx)
            httpx.RequestError: On network/connection errors
            LLMError: On API-specific errors
        """
        headers: dict[str, str] = {
            "Content-Type": "application/json",
            "x-api-key": self._api_key,  # type: ignore[dict-item]
            "anthropic-version": "2023-06-01",
        }

        async with httpx.AsyncClient(timeout=self._timeout) as client:
            response = await client.post(
                str(self._base_url),
                headers=headers,
                json=request_body,
            )
            response.raise_for_status()
            return response.json()  # type: ignore[no-any-return]

    def _format_http_error(self, error: httpx.HTTPStatusError) -> str:
        """
        Format HTTP error into a user-friendly message.

        Args:
            error: HTTPStatusError from the API call

        Returns:
            User-friendly error message
        """
        status = error.response.status_code

        if status == 401:
            return "Claude API authentication failed - check ANTHROPIC_API_KEY"
        elif status == 429:
            return "Claude API rate limit exceeded"
        elif status == 400:
            return "Claude API bad request - check request format"
        elif status == 529:
            return "Claude API service overloaded - please retry"
        elif status >= 500:
            return f"Claude API server error {status} - please retry"
        else:
            return f"Claude API error {status}: {error.response.text}"

    def _extract_response_text(self, response_json: dict) -> tuple[str, dict | None]:
        """
        Extract text content and usage from Claude API response.

        Claude response format:
        {
            "id": "msg_xxx",
            "type": "message",
            "role": "assistant",
            "content": [
                {"type": "text", "text": "Generated text here"}
            ],
            "model": "claude-3-5-sonnet-20241022",
            "stop_reason": "end_turn",
            "usage": {
                "input_tokens": 10,
                "output_tokens": 20
            }
        }

        Args:
            response_json: Parsed JSON response from Claude

        Returns:
            Tuple of (text content, usage dict or None)

        Raises:
            LLMError: If response structure is invalid
        """
        try:
            # Extract text from content blocks
            content_blocks = response_json["content"]
            text_parts = []

            for block in content_blocks:
                if block.get("type") == "text":
                    text_parts.append(block.get("text", ""))

            text = "".join(text_parts)

            # Extract usage information if available
            usage = response_json.get("usage")

            if not text:
                raise LLMError("Empty response from Claude API")

            return text, usage

        except (KeyError, IndexError) as e:
            raise LLMError(f"Invalid Claude response structure: {e}") from e


__all__ = [
    "ClaudeLLMClient",
    "DEFAULT_MODEL",
    "DEFAULT_TEMPERATURE",
    "DEFAULT_MAX_TOKENS",
    "CLAUDE_MODELS",
]
