"""
Enhanced Multi-Agent Bridge Modular Integration Tests
====================================================

Comprehensive tests for the modular enhanced multi-agent bridge implementation.
"""

import asyncio
import logging
from typing import Any

import pytest

# Import the modular bridge
from src.bridges.multi_agent_bridge import (
    CommunicationType,
    EnhancedMultiAgentBridge,
    LLMCoordinationConfig,
)

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


# Test fixtures and mocks
class MockEventBus:
    def __init__(self):
        self.events = []

    def emit(self, event_type: str, data: Any):
        self.events.append({"type": event_type, "data": data})


class MockPersonaAgent:
    def __init__(self, agent_id: str):
        self.agent_id = agent_id
        self.role = "Test Agent"
        self.personality = {"aggression": 0.5, "intelligence": 0.7}

    async def get_current_state(self):
        return {
            "agent_id": self.agent_id,
            "status": "active",
            "location": "test_location",
        }


class MockDirectorAgent:
    def __init__(self):
        self.turn_count = 0

    async def run_turn(self, turn_data):
        self.turn_count += 1
        return {"success": True, "turn_number": self.turn_count, "agents_processed": 2}


@pytest.mark.asyncio
async def test_bridge_initialization():
    """Test 1: Enhanced Multi-Agent Bridge Component Initialization"""
    print("\\nüîß TEST 1: Bridge Component Initialization")

    try:
        event_bus = MockEventBus()
        config = LLMCoordinationConfig(
            max_cost_per_turn=0.05, enable_smart_batching=True
        )

        bridge = EnhancedMultiAgentBridge(
            event_bus=event_bus, coordination_config=config, logger=logger
        )

        # Verify components are initialized
        assert bridge.cost_tracker is not None, "CostTracker should be initialized"
        assert (
            bridge.performance_budget is not None
        ), "PerformanceBudget should be initialized"
        assert (
            bridge.llm_processor is not None
        ), "LLMBatchProcessor should be initialized"
        assert (
            bridge.dialogue_manager is not None
        ), "DialogueManager should be initialized"
        assert (
            bridge.performance_metrics is not None
        ), "PerformanceMetrics should be initialized"

        # Verify configuration applied
        assert bridge.config.max_cost_per_turn == 0.05
        assert bridge.cost_tracker.max_cost_per_turn == 0.05

        print("‚úÖ Component initialization successful")
        print(
            f"   Components initialized: {len(bridge._integration_stats['component_status'])}"
        )
        return True

    except Exception as e:
        print(f"‚ùå Component initialization failed: {e}")
        return False


@pytest.mark.asyncio
async def test_ai_systems_initialization():
    """Test 2: AI Systems Integration"""
    print("\\nü§ñ TEST 2: AI Systems Integration")

    try:
        event_bus = MockEventBus()
        bridge = EnhancedMultiAgentBridge(event_bus)

        # Initialize AI systems
        result = await bridge.initialize_ai_systems()

        assert result["success"] is True, "AI systems initialization should succeed"
        assert bridge._is_initialized is True, "Bridge should be marked as initialized"
        assert (
            result["initialization_time"] is not None
        ), "Should record initialization time"

        print("‚úÖ AI systems integration successful")
        print(f"   Initialization time: {result['initialization_time']:.3f}s")
        return True

    except Exception as e:
        print(f"‚ùå AI systems integration failed: {e}")
        return False


@pytest.mark.asyncio
async def test_agent_registration():
    """Test 3: Agent Registration and Management"""
    print("\\nüë• TEST 3: Agent Registration and Management")

    try:
        event_bus = MockEventBus()
        bridge = EnhancedMultiAgentBridge(event_bus)
        await bridge.initialize_ai_systems()

        # Register test agents
        agent1 = MockPersonaAgent("agent_001")
        agent2 = MockPersonaAgent("agent_002")

        bridge.register_agent("agent_001", agent1)
        bridge.register_agent("agent_002", agent2)

        # Verify registration
        assert len(bridge._agents) == 2, "Should have 2 registered agents"
        assert "agent_001" in bridge._agents, "Agent 001 should be registered"
        assert "agent_002" in bridge._agents, "Agent 002 should be registered"

        # Test agent status retrieval
        status = await bridge.get_enhanced_agent_status("agent_001")
        assert status["agent_id"] == "agent_001", "Should return correct agent status"
        assert "basic_status" in status, "Should include basic status"

        print("‚úÖ Agent registration successful")
        print(f"   Registered agents: {list(bridge._agents.keys())}")
        return True

    except Exception as e:
        print(f"‚ùå Agent registration failed: {e}")
        return False


@pytest.mark.asyncio
async def test_dialogue_management():
    """Test 4: Dialogue Management System"""
    print("\\nüí¨ TEST 4: Dialogue Management System")

    try:
        event_bus = MockEventBus()
        bridge = EnhancedMultiAgentBridge(event_bus)
        await bridge.initialize_ai_systems()

        # Register agents
        agent1 = MockPersonaAgent("agent_001")
        agent2 = MockPersonaAgent("agent_002")
        bridge.register_agent("agent_001", agent1)
        bridge.register_agent("agent_002", agent2)

        # Test dialogue initiation
        dialogue_result = await bridge.initiate_agent_dialogue(
            initiator_id="agent_001",
            target_id="agent_002",
            communication_type=CommunicationType.DIALOGUE,
            context={"test": True},
        )

        assert (
            dialogue_result["success"] is True
        ), "Dialogue should be initiated successfully"
        assert "dialogue_id" in dialogue_result, "Should return dialogue ID"

        # Check dialogue stats
        dialogue_stats = bridge.dialogue_manager.get_dialogue_stats()
        assert (
            dialogue_stats["total_dialogues_initiated"] >= 1
        ), "Should track dialogue initiation"

        print("‚úÖ Dialogue management successful")
        print(
            f"   Dialogue result: {dialogue_result['result']['outcome'] if 'result' in dialogue_result else 'processed'}"
        )
        return True

    except Exception as e:
        print(f"‚ùå Dialogue management failed: {e}")
        return False


@pytest.mark.asyncio
async def test_enhanced_turn_execution():
    """Test 5: Enhanced Turn Execution"""
    print("\\nüîÑ TEST 5: Enhanced Turn Execution")

    try:
        event_bus = MockEventBus()
        director_agent = MockDirectorAgent()
        bridge = EnhancedMultiAgentBridge(
            event_bus=event_bus, director_agent=director_agent
        )
        await bridge.initialize_ai_systems()

        # Register agents
        agent1 = MockPersonaAgent("agent_001")
        agent2 = MockPersonaAgent("agent_002")
        bridge.register_agent("agent_001", agent1)
        bridge.register_agent("agent_002", agent2)

        # Execute enhanced turn
        turn_result = await bridge.enhanced_run_turn(
            {"turn_number": 1, "world_state": {"location": "test_area"}}
        )

        assert turn_result["success"] is True, "Enhanced turn should succeed"
        assert turn_result["turn_number"] == 1, "Should track turn number"
        assert "dialogue_results" in turn_result, "Should include dialogue results"
        assert "performance_data" in turn_result, "Should include performance data"
        assert "enhanced_summary" in turn_result, "Should generate enhanced summary"

        print("‚úÖ Enhanced turn execution successful")
        print(f"   Turn time: {turn_result['total_time']:.3f}s")
        print(f"   Coordinations: {turn_result['coordination_count']}")
        print(f"   Summary: {turn_result['enhanced_summary']}")
        return True

    except Exception as e:
        print(f"‚ùå Enhanced turn execution failed: {e}")
        return False


@pytest.mark.asyncio
async def test_performance_tracking():
    """Test 6: Performance Metrics and Tracking"""
    print("\\nüìä TEST 6: Performance Metrics and Tracking")

    try:
        event_bus = MockEventBus()
        bridge = EnhancedMultiAgentBridge(event_bus)
        await bridge.initialize_ai_systems()

        # Register agents
        bridge.register_agent("agent_001", MockPersonaAgent("agent_001"))
        bridge.register_agent("agent_002", MockPersonaAgent("agent_002"))

        # Execute a turn to generate metrics
        await bridge.enhanced_run_turn({"turn_number": 1})

        # Get comprehensive metrics
        metrics = bridge.get_coordination_performance_metrics()

        assert "cost_efficiency" in metrics, "Should include cost efficiency metrics"
        assert "performance_stats" in metrics, "Should include performance statistics"
        assert "system_health_score" in metrics, "Should calculate system health score"

        # Test system intelligence dashboard
        dashboard = await bridge.get_system_intelligence_dashboard()

        assert (
            dashboard["system_status"] == "operational"
        ), "System should be operational"
        assert "performance_metrics" in dashboard, "Should include performance metrics"
        assert "integration_stats" in dashboard, "Should include integration statistics"

        print("‚úÖ Performance tracking successful")
        print(f"   System health: {metrics.get('system_health_score', 0):.1%}")
        print(
            f"   Total turns processed: {bridge._integration_stats['total_turns_processed']}"
        )
        return True

    except Exception as e:
        print(f"‚ùå Performance tracking failed: {e}")
        return False


@pytest.mark.asyncio
async def test_cost_and_budget_management():
    """Test 7: Cost and Budget Management"""
    print("\\nüí∞ TEST 7: Cost and Budget Management")

    try:
        event_bus = MockEventBus()
        config = LLMCoordinationConfig(max_cost_per_turn=0.05)
        bridge = EnhancedMultiAgentBridge(event_bus, coordination_config=config)
        await bridge.initialize_ai_systems()

        # Test cost tracking
        initial_cost = bridge.cost_tracker.current_turn_cost
        assert initial_cost == 0.0, "Initial turn cost should be zero"

        # Simulate cost update
        within_budget = bridge.cost_tracker.update_cost("test", 0.02, 100)
        assert within_budget is True, "Should be within budget"

        # Test budget checks
        budget_ok = bridge.cost_tracker.is_under_budget(0.02)
        assert budget_ok is True, "Should have budget available"

        # Test budget exceeded
        over_budget = bridge.cost_tracker.update_cost("test", 0.10, 500)
        assert over_budget is False, "Should detect budget exceeded"

        # Test performance budget
        bridge.performance_budget.start_turn()
        remaining_time = bridge.performance_budget.get_remaining_time()
        assert remaining_time > 0, "Should have remaining time budget"

        print("‚úÖ Cost and budget management successful")
        print(f"   Current cost: ${bridge.cost_tracker.current_turn_cost:.4f}")
        print(
            f"   Remaining budget: ${bridge.cost_tracker.get_remaining_turn_budget():.4f}"
        )
        return True

    except Exception as e:
        print(f"‚ùå Cost and budget management failed: {e}")
        return False


@pytest.mark.asyncio
async def test_component_integration():
    """Test 8: Component Integration and Communication"""
    print("\\nüîó TEST 8: Component Integration and Communication")

    try:
        event_bus = MockEventBus()
        bridge = EnhancedMultiAgentBridge(event_bus)
        await bridge.initialize_ai_systems()

        # Test component communication
        # Performance metrics should use cost tracker and performance budget
        metrics = bridge.performance_metrics
        cost_stats = metrics.cost_tracker.get_cost_efficiency_stats()
        perf_stats = metrics.performance_budget.get_performance_stats()

        assert cost_stats is not None, "Performance metrics should access cost tracker"
        assert (
            perf_stats is not None
        ), "Performance metrics should access performance budget"

        # Test LLM processor integration with cost and performance components
        processor = bridge.llm_processor
        assert (
            processor.cost_tracker == bridge.cost_tracker
        ), "LLM processor should use same cost tracker"
        assert (
            processor.performance_budget == bridge.performance_budget
        ), "LLM processor should use same performance budget"

        # Test dialogue manager integration with LLM processor
        dialogue_mgr = bridge.dialogue_manager
        assert (
            dialogue_mgr.llm_processor == bridge.llm_processor
        ), "Dialogue manager should use LLM processor"

        print("‚úÖ Component integration successful")
        print("   Components properly connected and communicating")
        return True

    except Exception as e:
        print(f"‚ùå Component integration failed: {e}")
        return False


@pytest.mark.asyncio
async def test_backward_compatibility():
    """Test 9: Backward Compatibility"""
    print("\\nüîÑ TEST 9: Backward Compatibility")

    try:
        event_bus = MockEventBus()
        bridge = EnhancedMultiAgentBridge(event_bus)
        await bridge.initialize_ai_systems()

        # Test legacy method access through __getattr__
        try:
            # This should work through the legacy mapping system
            stats_method = bridge.get_performance_stats
            assert callable(
                stats_method
            ), "Should provide access to performance stats method"
        except AttributeError:
            # Expected if no legacy mapping exists yet, this is fine
            pass

        # Test factory function
        from src.bridges.multi_agent_bridge.enhanced_multi_agent_bridge_modular import (
            create_enhanced_multi_agent_bridge,
            create_performance_optimized_config,
        )

        # Test configuration factory
        optimized_config = create_performance_optimized_config(
            max_turn_time_seconds=3.0, max_cost_per_turn=0.08
        )
        assert (
            optimized_config.max_cost_per_turn == 0.08
        ), "Should create optimized config"

        # Test bridge factory
        factory_bridge = create_enhanced_multi_agent_bridge(
            event_bus=event_bus, coordination_config=optimized_config
        )
        assert isinstance(
            factory_bridge, EnhancedMultiAgentBridge
        ), "Should create bridge instance"

        print("‚úÖ Backward compatibility successful")
        print("   Legacy interfaces and factory functions working")
        return True

    except Exception as e:
        print(f"‚ùå Backward compatibility failed: {e}")
        return False


async def run_comprehensive_test():
    """Run all integration tests for enhanced multi-agent bridge modularization."""
    print(
        "üöÄ STARTING COMPREHENSIVE ENHANCED MULTI-AGENT BRIDGE MODULAR INTEGRATION TESTS"
    )
    print("=" * 80)

    test_functions = [
        test_bridge_initialization,
        test_ai_systems_initialization,
        test_agent_registration,
        test_dialogue_management,
        test_enhanced_turn_execution,
        test_performance_tracking,
        test_cost_and_budget_management,
        test_component_integration,
        test_backward_compatibility,
    ]

    results = []
    passed_tests = 0

    for i, test_func in enumerate(test_functions, 1):
        try:
            result = await test_func()
            results.append(result)
            if result:
                passed_tests += 1
        except Exception as e:
            print(f"‚ùå TEST {i} CRITICAL FAILURE: {e}")
            results.append(False)

    print("\\n" + "=" * 80)
    print("üìä TEST SUMMARY")
    print(f"Total Tests: {len(test_functions)}")
    print(f"Passed: {passed_tests}")
    print(f"Failed: {len(test_functions) - passed_tests}")
    print(f"Success Rate: {(passed_tests/len(test_functions)*100):.1f}%")

    if passed_tests == len(test_functions):
        print(
            "üéâ Enhanced Multi-Agent Bridge modular integration: ROBUST IMPLEMENTATION"
        )
        print("   All 9 specialized components working in perfect harmony")
        print("   ‚ú® Enterprise-grade modular architecture validated")
    else:
        print("‚ö†Ô∏è Some issues detected - modular architecture needs attention")

    return passed_tests, len(test_functions)


if __name__ == "__main__":
    asyncio.run(run_comprehensive_test())
