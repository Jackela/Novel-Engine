/**
 * LLM Mock Infrastructure for E2E Testing
 *
 * This module provides mock handlers for LLM-related API endpoints.
 * When enabled, it intercepts expensive LLM API calls and returns
 * dummy JSON instantly, dramatically speeding up E2E tests.
 *
 * Why: Real LLM calls are expensive (cost + time). For E2E tests,
 * we need to verify UI behavior, not LLM quality. This mock allows
 * tests to run quickly (<2 seconds) while maintaining test coverage.
 *
 * Usage:
 *   import { mockLLMEndpoints, isMockLLMEnabled } from './utils/llmMocks';
 *
 *   if (isMockLLMEnabled()) {
 *     await mockLLMEndpoints(page);
 *   }
 *
 * Enable via environment variable: MOCK_LLM=true
 */
import type { Page } from '@playwright/test';

/**
 * Mock response data for character generation.
 */
const MOCK_CHARACTER_RESPONSE = {
  name: 'Mock Character',
  tagline: 'A test character for E2E validation.',
  bio: 'This character was generated by the LLM mock infrastructure for testing purposes.',
  visual_prompt: 'test character, simple design, placeholder aesthetic',
  traits: ['mock', 'test', 'placeholder'],
};

/**
 * Mock response data for scene generation.
 */
const MOCK_SCENE_RESPONSE = {
  title: 'Mock Scene',
  content:
    'This is a mock scene generated for testing. The characters moved through the environment with purpose.',
  summary: 'A test scene demonstrating mock LLM infrastructure.',
  visual_prompt: 'test scene, simple environment, placeholder aesthetic',
};

/**
 * Mock response data for generic generation endpoint.
 */
const MOCK_GENERATE_RESPONSE = {
  success: true,
  content: 'Mock generated content for testing.',
  metadata: {
    model: 'mock-llm-v1',
    tokens_used: 0,
    duration_ms: 10,
  },
};

/**
 * Checks if LLM mocking is enabled via environment variable.
 * Returns true when MOCK_LLM environment variable is set to 'true'.
 */
export function isMockLLMEnabled(): boolean {
  return process.env.MOCK_LLM === 'true';
}

/**
 * Creates mock SSE (Server-Sent Events) response body for streaming endpoints.
 * Returns immediately with minimal synthetic delay.
 */
function createMockSSEResponse(chunks: string[]): string {
  let sseBody = '';
  for (const chunk of chunks) {
    sseBody += `data: ${JSON.stringify({ type: 'chunk', content: chunk })}\n\n`;
  }
  sseBody += `data: ${JSON.stringify({ type: 'done', content: '' })}\n\n`;
  return sseBody;
}

/**
 * Mocks the character generation endpoint.
 * Intercepts: POST /api/generation/character
 */
export async function mockCharacterGeneration(page: Page): Promise<void> {
  await page.route('**/api/generation/character', async (route) => {
    await route.fulfill({
      status: 200,
      contentType: 'application/json',
      body: JSON.stringify(MOCK_CHARACTER_RESPONSE),
    });
  });
}

/**
 * Mocks the scene generation endpoint.
 * Intercepts: POST /api/generation/scene
 */
export async function mockSceneGeneration(page: Page): Promise<void> {
  await page.route('**/api/generation/scene', async (route) => {
    await route.fulfill({
      status: 200,
      contentType: 'application/json',
      body: JSON.stringify(MOCK_SCENE_RESPONSE),
    });
  });
}

/**
 * Mocks the narrative streaming endpoint.
 * Intercepts: POST /api/narratives/stream
 */
export async function mockNarrativeStream(page: Page): Promise<void> {
  await page.route('**/api/narratives/stream', async (route) => {
    const mockChunks = [
      'Mock narrative text. ',
      'The story unfolds quickly. ',
      'All characters act as expected.',
    ];
    await route.fulfill({
      status: 200,
      contentType: 'text/event-stream',
      body: createMockSSEResponse(mockChunks),
    });
  });
}

/**
 * Mocks the generic generation endpoint.
 * Intercepts: POST /api/generate
 */
export async function mockGenerateEndpoint(page: Page): Promise<void> {
  await page.route('**/api/generate', async (route) => {
    await route.fulfill({
      status: 200,
      contentType: 'application/json',
      body: JSON.stringify(MOCK_GENERATE_RESPONSE),
    });
  });
}

/**
 * Mocks the orchestration narrative endpoint.
 * Intercepts: GET/POST /api/orchestration/narrative
 */
export async function mockOrchestrationNarrative(page: Page): Promise<void> {
  await page.route('**/api/orchestration/narrative', async (route) => {
    await route.fulfill({
      status: 200,
      contentType: 'application/json',
      body: JSON.stringify({
        success: true,
        data: {
          story: 'Mock orchestration narrative output.',
          participants: ['Character A', 'Character B'],
          turns_completed: 1,
          last_generated: new Date().toISOString(),
          has_content: true,
        },
      }),
    });
  });
}

/**
 * Mocks ALL LLM-related endpoints for comprehensive E2E testing.
 * This is the primary entry point for enabling LLM mocks.
 *
 * Why mock all endpoints: E2E tests often navigate across features
 * that may call different LLM endpoints. Mocking all prevents
 * unexpected timeouts or costs during comprehensive test runs.
 */
export async function mockLLMEndpoints(page: Page): Promise<void> {
  await Promise.all([
    mockGenerateEndpoint(page),
    mockCharacterGeneration(page),
    mockSceneGeneration(page),
    mockNarrativeStream(page),
    mockOrchestrationNarrative(page),
  ]);
}

/**
 * Conditionally applies LLM mocks based on MOCK_LLM environment variable.
 * Use this as a drop-in helper in test setup.
 *
 * Example:
 *   test.beforeEach(async ({ page }) => {
 *     await applyLLMMocksIfEnabled(page);
 *   });
 */
export async function applyLLMMocksIfEnabled(page: Page): Promise<boolean> {
  if (isMockLLMEnabled()) {
    await mockLLMEndpoints(page);
    return true;
  }
  return false;
}
