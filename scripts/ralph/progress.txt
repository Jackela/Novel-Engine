# Ralph Progress Log
Started: Fri Feb  6 19:52:52 CST 2026
---

## [2026-02-06] - OPT-012
- Implemented local model prompt strategy for DeepSeek, Llama, and other local models
- Files changed:
  - src/contexts/knowledge/domain/models/model_registry.py - Added PromptModelFamily and PromptFormat enums
  - src/contexts/knowledge/application/services/model_registry.py - Added DeepSeek, Llama 3.x, Codestral, Phi, Qwen models
  - src/contexts/knowledge/application/services/prompt_formatter.py - NEW: Prompt formatting service
  - tests/unit/contexts/knowledge/application/services/test_prompt_formatter.py - NEW: 38 tests for prompt formatting
  - src/contexts/knowledge/application/services/__init__.py - Exported new types and service
- Prompt formats implemented:
  - CHAT_MESSAGES: Role-based messages (OpenAI, Claude, Gemini)
  - INSTRUCTION: Alpaca-style instruction format (Llama, Mistral, Phi, Qwen)
  - CODE_INSTRUCTION: DeepSeek-Coder specific format
  - COMPLETION: Simple completion format
- Auto-detection: ModelDefinition auto-detects family from model name and sets appropriate prompt format
- Model aliases added: deepseek, coder, llama, phi, qwen, codestral, and more

**Learnings for future iterations:**
- The token_counter module already has a ModelFamily enum for tokenizer selection - renamed my enum to PromptModelFamily to avoid conflicts
- Global find-replace operations can cause double-prefix issues (ModelFamily -> PromptModelFamily -> PromptPromptModelFamily) - need to be careful with consecutive replacements
- Domain models directory is in .gitignore - need to use `git add -f` to commit files from there
- The existing prompt template system (PromptTemplate with {{variable}} syntax) is separate from prompt formatting - the new PromptFormatter handles the outer structure (instruction vs chat format) while PromptTemplate handles variable substitution
- Model auto-detection in __post_init__ with frozen dataclass requires using object.__setattr__(self, "field", value)

---

## [2026-02-06] - OPT-013
- Created RAG Pipeline Architecture Diagram for onboarding and reviews
- Files changed:
  - docs/architecture/rag_pipeline.mermaid - NEW: Complete RAG pipeline visualization
  - docs/index.md - Added link to RAG diagram in Architecture section
  - README.md - Added link to RAG diagram with Chinese description
  - README.en.md - Added link to RAG diagram with English description
- Diagram covers the complete pipeline:
  - Ingestion Phase: Raw Text -> KnowledgeIngestionService -> TextChunker -> Chunked Document
  - Embedding Phase: EmbeddingCacheService -> EmbeddingServiceAdapter -> Embedded Chunks
  - Storage Phase: ChromaDBVectorStore -> Vector Store
  - Retrieval Phase: RetrievalService + HybridRetriever + BM25Retriever -> Candidate Results
  - Reranking Phase: RerankService -> Reranked Results (with fallback)
  - Generation Phase: format_context -> Formatted Context -> LLM Generation
- Component details included as HTML comments for onboarding reference

**Learnings for future iterations:**
- Mermaid subgraph syntax requires `direction TB` (top-bottom) or `direction LR` for proper layout
- Use different node shapes: `[Rectangle]` for services, `[("Rounded")]` for data stores/states
- Styling uses `classDef` and `class` keywords for color coding different phases
- The existing docs/architecture/ directory already has narrative_engine.mermaid and world_engine.mermaid as examples
- README.md is Chinese, README.en.md is English - both need to be updated for documentation changes
- docs/index.md uses the format "### [Architecture](./architecture/) ‚≠ê" - add new items under existing sections

---
