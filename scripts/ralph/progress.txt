# Ralph Progress Log - Warzone 4: AI Brain
Started: 2025-02-03

Campaign: WARZONE 4 - Building the RAG (Retrieval-Augmented Generation) engine
Branch: feat/code-citadel
Stories: 40 (BRAIN-001 through BRAIN-040)

---

## Codebase Patterns

### Architecture
- **Hexagonal Architecture**: Routers -> Services -> Domain. No business logic in routers.
- **Result Pattern**: Use `Result[T, E]` for error handling in services (see `src/core/result.py`)
- **Domain Events**: Use events for cross-context communication, not direct imports
- **Import Linting**: Rules defined in `.importlinter` to enforce architectural boundaries
- **Vector Storage**: ChromaDB adapter in `src/contexts/knowledge/infrastructure/adapters/chromadb_vector_store.py`
- **Type Annotations**: Use `from __future__ import annotations` and import List/Tuple from typing for complex types

### Frontend
- **Zustand Stores**: Feature-based organization (4 stores: authStore, orchestrationStore, decisionStore, weaverStore)
- **State Management Hierarchy**: TanStack Query -> Zustand -> React Hook Form -> useState
- **Schema SSOT**: Backend Pydantic schemas (`src/api/schemas.py`) drive frontend Zod schemas (`frontend/src/types/schemas.ts`)

### Testing
- **TDD/BDD**: Write failing test first, implement minimum code, verify with E2E
- **Test File Organization**: Keep files under 500 lines, split by functionality
- **Quality Gates**: `pytest tests/ && npm run test:e2e && mypy . && npm run type-check`

### Schema Synchronization
- Backend schemas are SSOT in `src/api/schemas.py`
- Frontend schemas live in `frontend/src/types/schemas.ts` (Zod)
- Regenerate OpenAPI with `python scripts/generate_openapi.py` after schema changes
- Zod's `z.number()` is correct for both Python `int` and `float`

---

## Warzone 4: AI Brain - Campaign Overview

### Goal
Build a RAG (Retrieval-Augmented Generation) engine and Prompt Engineering Lab to give the AI system "long-term memory."

### Key Technologies
- **Vector Database**: ChromaDB for semantic search
- **Embeddings**: OpenAI `text-embedding-3-small` (1536 dimensions)
- **LLM Providers**: OpenAI, Anthropic Claude, Google Gemini, Ollama (local)
- **Knowledge Graph**: NetworkX/Neo4j for entity relationships

### Story Breakdown

#### Core Infrastructure (7 stories)
- BRAIN-001: ChromaDB setup ✅
- BRAIN-002: Embedding service ✅
- BRAIN-003: Knowledge entry entity ✅
- BRAIN-004: Knowledge ingestion service
- BRAIN-005: Auto-sync event listeners
- BRAIN-006: Context retrieval service (RAG)
- BRAIN-007: RAG-enhanced generation

#### RAG Enhancement (6 stories)
- BRAIN-008: Hybrid search (Vector + BM25)
- BRAIN-009: Query rewriting & expansion
- BRAIN-010: Re-ranking service
- BRAIN-011: Context window optimization
- BRAIN-012: Citation & source attribution
- BRAIN-013: Multi-hop reasoning

#### Prompt Engineering (9 stories)
- BRAIN-014: Prompt template entity
- BRAIN-015: Prompt management API
- BRAIN-016: Version control & history
- BRAIN-017: Template inheritance & composition
- BRAIN-018: A/B testing framework
- BRAIN-019: Prompt Engineering Lab UI
- BRAIN-020: Prompt playground
- BRAIN-021: Prompt comparison view
- BRAIN-022: Prompt analytics

#### LLM Integration (6 stories)
- BRAIN-023: Model registry
- BRAIN-024: Anthropic Claude integration
- BRAIN-025: Google Gemini integration
- BRAIN-026: OpenAI GPT-4 Turbo integration
- BRAIN-027: Local LLM support (Ollama)
- BRAIN-028: Model routing strategy

#### Knowledge Graph (4 stories)
- BRAIN-029: Entity extraction service
- BRAIN-030: Relationship extraction
- BRAIN-031: Graph storage (NetworkX/Neo4j)
- BRAIN-032: Graph-based context retrieval

#### Settings & Monitoring (3 stories)
- BRAIN-033: Brain settings UI
- BRAIN-034: Token usage tracker
- BRAIN-035: Cost dashboard

#### Features (5 stories)
- BRAIN-036: Context inspector
- BRAIN-037: Chat with Story
- BRAIN-038: Smart tagging service
- BRAIN-039: Automatic chunking strategy
- BRAIN-040: E2E Brain Test

---

## [2025-02-03] - BRAIN-001 ✅
- Implemented ChromaDB vector store adapter with full CRUD operations
- Created IVectorStore port interface with upsert, query, delete, clear, health_check, count methods
- Added ChromaDB health check endpoint at GET /health/chromadb
- Added register_chromadb_check method to HealthMonitor for system-wide monitoring
- All 23 unit tests passing, including data persistence across restarts test
- Files changed:
  - `pyproject.toml`: Added chromadb>=0.5.0 and tiktoken>=0.7.0 dependencies
  - `src/contexts/knowledge/application/ports/i_vector_store.py`: IVectorStore port, VectorDocument, QueryResult, UpsertResult value objects
  - `src/contexts/knowledge/infrastructure/adapters/chromadb_vector_store.py`: ChromaDBVectorStore adapter
  - `src/api/health_system.py`: Added register_chromadb_check method
  - `src/api/routers/health.py`: Added /health/chromadb endpoint
  - `tests/unit/contexts/knowledge/infrastructure/adapters/test_chromadb_vector_store.py`: Full test coverage

**Learnings for future iterations:**
- ChromaDB uses PersistentClient for embedded mode - no separate server needed
- Default storage is `.data/chroma/` - configurable via CHROMA_PERSIST_DIR env var
- Collection cache must be instance-level (not class-level) to avoid cross-test pollution
- Cosine similarity is the default distance metric for text embeddings
- ChromaDB's `query` returns distances that need to be converted to similarity scores (1 - distance)

---

## [2025-02-03] - BRAIN-002 ✅
- Implemented EmbeddingService adapter with OpenAI embeddings API integration
- Created IEmbeddingService port interface with embed, embed_batch, get_dimension, clear_cache methods
- Added real OpenAI embeddings support (text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002)
- Implemented deterministic fallback mock embeddings for testing
- Added batch embedding support with caching for efficiency
- Maintained backward compatibility with EmbeddingGeneratorAdapter alias
- All 32 unit tests passing
- Files changed:
  - `pyproject.toml`: Added openai>=1.0.0,<2.0.0 dependency
  - `src/contexts/knowledge/application/ports/i_embedding_service.py`: IEmbeddingService port, EmbeddingError exception
  - `src/contexts/knowledge/infrastructure/adapters/embedding_generator_adapter.py`: Complete rewrite of EmbeddingServiceAdapter
  - `tests/unit/contexts/knowledge/infrastructure/adapters/test_embedding_service_adapter.py`: Full test coverage

**Learnings for future iterations:**
- OpenAI's AsyncOpenAI client requires lazy loading to avoid import errors when package isn't installed
- Use TYPE_CHECKING to import types only for mypy, not runtime
- Mock embeddings use text hash for determinism - same text always produces same vector
- Embeddings should be normalized to unit length (standard practice for semantic search)
- Caching at the adapter level avoids redundant API calls for identical text
- Batch mode can be more efficient but should still respect cache for individual items

---

## [2025-02-03] - BRAIN-003 ✅
- Created ChunkingStrategy value object with FIXED/SEMANTIC/SENTENCE/PARAGRAPH strategies
  - Configurable chunk_size (50-2000), overlap, and min_chunk_size
  - Presets for character, scene, and lore content types
- Created SourceType enumeration (CHARACTER, LORE, SCENE, PLOTLINE, ITEM, LOCATION)
- Created SourceKnowledgeEntry entity for RAG with embedding tracking
- Implemented TextChunker domain service with multiple chunking strategies
- Added metadata enrichment with word_count, chunk_index, total_chunks, tags
- All 68 unit tests passing
- Files changed:
  - `src/contexts/knowledge/domain/models/chunking_strategy.py`: ChunkingStrategy VO, ChunkStrategyType enum, ChunkingStrategies factory
  - `src/contexts/knowledge/domain/models/source_type.py`: SourceType enum with from_string()
  - `src/contexts/knowledge/domain/models/source_knowledge_entry.py`: SourceKnowledgeEntry entity, SourceMetadata VO
  - `src/contexts/knowledge/domain/services/text_chunker.py`: TextChunker with FIXED/SENTENCE/PARAGRAPH/SEMANTIC strategies
  - `tests/unit/contexts/knowledge/domain/models/`: Tests for chunking_strategy, source_type, source_knowledge_entry
  - `tests/unit/contexts/knowledge/domain/services/test_text_chunker.py`: 18 tests for all chunking strategies

**Learnings for future iterations:**
- Use `from __future__ import annotations` to enable forward references in enum classmethod return types
- The domain/models directory is gitignored - use `git add -f` to add new model files
- Chunking with overlap means total_words sum includes overlaps (2000 words + overlap = 2200 total_words)
- TextChunker._WORD_PATTERN.findall() is the reliable way to count words (handles irregular spacing)
- For paragraph chunking, exclude delimiter length when calculating text position: end_pos - len(match.group())

---

## Next Story for Ralph
- **BRAIN-005**: Integration: Auto-Sync Event Listeners
- Priority: 5
- Focus: Create domain event handlers to trigger ingestion on CharacterCreated/LoreCreated/etc.
- Dependencies: BRAIN-004 (KnowledgeIngestionService)

---
- Implemented KnowledgeIngestionService with full ingestion pipeline (Text -> Chunk -> Embed -> Store)
- Created `ingest()` method for single entry ingestion with configurable chunking strategy
- Created `batch_ingest()` method for bulk processing with progress tracking via IngestionProgress callback
- Created `delete()` method to remove all chunks for a source with optional source_type filter
- Created `update()` method to replace old chunks with new content (delete + ingest)
- Created `query_by_source()` method to retrieve all chunks for a given source
- Added health_check() and get_count() utility methods
- Implemented value objects: IngestionProgress, IngestionResult, BatchIngestionResult, SourceChunk, RetrievedChunk
- Files changed:
  - `src/contexts/knowledge/application/services/knowledge_ingestion_service.py`: Complete service implementation
  - `src/contexts/knowledge/application/services/__init__.py`: Service module exports
  - `tests/unit/contexts/knowledge/application/services/test_knowledge_ingestion_service.py`: 23 comprehensive tests

**Learnings for future iterations:**
- Use `embed_batch()` for efficiency instead of calling `embed()` in a loop
- The service layer should use `TYPE_CHECKING` for imports that are only needed for type hints
- Progress callbacks can be sync or async - use `inspect.iscoroutinefunction()` to detect
- Mock vector stores need to track state properly for delete operations to be testable
- ChunkingStrategy.from_string() normalizes string inputs to SourceType enum
- VectorDocument metadata is the place to store source tracking info (source_id, source_type, chunk_index)

---

## [2025-02-03] - BRAIN-005 ✅
- Implemented KnowledgeSyncEventHandler with async queue processing for knowledge ingestion
- Created KnowledgeEventSubscriber to bridge domain events (CharacterCreated/Updated, LoreCreated/Updated, SceneCreated/Updated) to ingestion
- Implemented async queue with configurable max_queue_size to avoid blocking
- Added retry logic with exponential backoff (configurable: FIXED/EXPONENTIAL/NONE)
- Implemented dead letter queue for failed tasks with manual retry capability
- Added content formatting helpers: _character_to_content, _lore_to_content, _scene_to_content
- Created IngestionTask value object for tracking retry state
- All 36 unit tests passing (14 for subscriber, 22 for handler)
- Files changed:
  - `src/contexts/knowledge/application/event_handlers/knowledge_sync_event_handler.py`: Main handler with queue, retry, dead letter
  - `src/contexts/knowledge/application/event_handlers/knowledge_event_subscriber.py`: Event subscriber bridging events to ingestion
  - `src/contexts/knowledge/application/event_handlers/__init__.py`: Package exports
  - `tests/unit/contexts/knowledge/application/event_handlers/test_knowledge_sync_event_handler.py`: 22 handler tests
  - `tests/unit/contexts/knowledge/application/event_handlers/test_knowledge_event_subscriber.py`: 14 subscriber tests

**Learnings for future iterations:**
- Async queue processing requires proper event loop management - asyncio.create_task() must be tracked for cleanup
- Use `asyncio.create_task()` with callback to track pending tasks: `task.add_done_callback(lambda t: self._pending_retry_tasks.discard(t))`
- On handler stop, gather all pending retry tasks with `asyncio.gather(*tasks, return_exceptions=True)` for clean shutdown
- asyncio.Queue(0) doesn't work as expected for testing full queue - it's a special synchronous queue
- For testing async handlers with queues, use short delays and queue.join() to wait for processing
- Test dead letter queue by setting max_retries=0 to skip the retry delay
- Content formatting helpers should include all relevant metadata (IDs, categories, tags) for RAG context

---
