# Ralph Progress Log
Started: Fri Feb  6 19:52:52 CST 2026
---

## [2026-02-06] - OPT-012
- Implemented local model prompt strategy for DeepSeek, Llama, and other local models
- Files changed:
  - src/contexts/knowledge/domain/models/model_registry.py - Added PromptModelFamily and PromptFormat enums
  - src/contexts/knowledge/application/services/model_registry.py - Added DeepSeek, Llama 3.x, Codestral, Phi, Qwen models
  - src/contexts/knowledge/application/services/prompt_formatter.py - NEW: Prompt formatting service
  - tests/unit/contexts/knowledge/application/services/test_prompt_formatter.py - NEW: 38 tests for prompt formatting
  - src/contexts/knowledge/application/services/__init__.py - Exported new types and service
- Prompt formats implemented:
  - CHAT_MESSAGES: Role-based messages (OpenAI, Claude, Gemini)
  - INSTRUCTION: Alpaca-style instruction format (Llama, Mistral, Phi, Qwen)
  - CODE_INSTRUCTION: DeepSeek-Coder specific format
  - COMPLETION: Simple completion format
- Auto-detection: ModelDefinition auto-detects family from model name and sets appropriate prompt format
- Model aliases added: deepseek, coder, llama, phi, qwen, codestral, and more

**Learnings for future iterations:**
- The token_counter module already has a ModelFamily enum for tokenizer selection - renamed my enum to PromptModelFamily to avoid conflicts
- Global find-replace operations can cause double-prefix issues (ModelFamily -> PromptModelFamily -> PromptPromptModelFamily) - need to be careful with consecutive replacements
- Domain models directory is in .gitignore - need to use `git add -f` to commit files from there
- The existing prompt template system (PromptTemplate with {{variable}} syntax) is separate from prompt formatting - the new PromptFormatter handles the outer structure (instruction vs chat format) while PromptTemplate handles variable substitution
- Model auto-detection in __post_init__ with frozen dataclass requires using object.__setattr__(self, "field", value)

---
