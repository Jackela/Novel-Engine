# Ralph Progress Log - Warzone 4: AI Brain
Started: 2025-02-03

Campaign: WARZONE 4 - Building the RAG (Retrieval-Augmented Generation) engine
Branch: feat/code-citadel
Stories: 40 (BRAIN-001 through BRAIN-040)

---

## Codebase Patterns

### Architecture
- **Hexagonal Architecture**: Routers -> Services -> Domain. No business logic in routers.
- **Result Pattern**: Use `Result[T, E]` for error handling in services (see `src/core/result.py`)
- **Domain Events**: Use events for cross-context communication, not direct imports
- **Import Linting**: Rules defined in `.importlinter` to enforce architectural boundaries
- **Vector Storage**: ChromaDB adapter in `src/contexts/knowledge/infrastructure/adapters/chromadb_vector_store.py`
- **Type Annotations**: Use `from __future__ import annotations` and import List/Tuple from typing for complex types
- **RAG Retrieval**: RetrievalService in `src/contexts/knowledge/application/services/retrieval_service.py` with filtering and deduplication
- **BM25 Keyword Search**: BM25Retriever in `src/contexts/knowledge/application/services/bm25_retriever.py` for exact keyword matching
- **Hybrid Retrieval**: HybridRetriever in `src/contexts/knowledge/application/services/hybrid_retriever.py` combines vector and BM25 with RRF

### Frontend
- **Zustand Stores**: Feature-based organization (4 stores: authStore, orchestrationStore, decisionStore, weaverStore)
- **State Management Hierarchy**: TanStack Query -> Zustand -> React Hook Form -> useState
- **Schema SSOT**: Backend Pydantic schemas (`src/api/schemas.py`) drive frontend Zod schemas (`frontend/src/types/schemas.ts`)

### Testing
- **TDD/BDD**: Write failing test first, implement minimum code, verify with E2E
- **Test File Organization**: Keep files under 500 lines, split by functionality
- **Quality Gates**: `pytest tests/ && npm run test:e2e && mypy . && npm run type-check`

### Schema Synchronization
- Backend schemas are SSOT in `src/api/schemas.py`
- Frontend schemas live in `frontend/src/types/schemas.ts` (Zod)
- Regenerate OpenAPI with `python scripts/generate_openapi.py` after schema changes
- Zod's `z.number()` is correct for both Python `int` and `float`

---

## Warzone 4: AI Brain - Campaign Overview

### Goal
Build a RAG (Retrieval-Augmented Generation) engine and Prompt Engineering Lab to give the AI system "long-term memory."

### Key Technologies
- **Vector Database**: ChromaDB for semantic search
- **Embeddings**: OpenAI `text-embedding-3-small` (1536 dimensions)
- **LLM Providers**: OpenAI, Anthropic Claude, Google Gemini, Ollama (local)
- **Knowledge Graph**: NetworkX/Neo4j for entity relationships

### Story Breakdown

#### Core Infrastructure (7 stories)
- BRAIN-001: ChromaDB setup ✅
- BRAIN-002: Embedding service ✅
- BRAIN-003: Knowledge entry entity ✅
- BRAIN-004: Knowledge ingestion service ✅
- BRAIN-005: Auto-sync event listeners ✅
- BRAIN-006: Context retrieval service (RAG) ✅
- BRAIN-007: RAG-enhanced generation ✅

#### RAG Enhancement (6 stories)
- BRAIN-008A: Hybrid Search - BM25 Implementation ✅
- BRAIN-008B: Hybrid Search - Score Fusion
- BRAIN-009: Query rewriting & expansion
- BRAIN-010: Re-ranking service
- BRAIN-011: Context window optimization
- BRAIN-012: Citation & source attribution
- BRAIN-013: Multi-hop reasoning

#### Prompt Engineering (9 stories)
- BRAIN-014: Prompt template entity
- BRAIN-015: Prompt management API
- BRAIN-016: Version control & history
- BRAIN-017: Template inheritance & composition
- BRAIN-018: A/B testing framework
- BRAIN-019: Prompt Engineering Lab UI
- BRAIN-020: Prompt playground
- BRAIN-021: Prompt comparison view
- BRAIN-022: Prompt analytics

#### LLM Integration (6 stories)
- BRAIN-023: Model registry
- BRAIN-024: Anthropic Claude integration
- BRAIN-025: Google Gemini integration
- BRAIN-026: OpenAI GPT-4 Turbo integration
- BRAIN-027: Local LLM support (Ollama)
- BRAIN-028: Model routing strategy

#### Knowledge Graph (4 stories)
- BRAIN-029: Entity extraction service
- BRAIN-030: Relationship extraction
- BRAIN-031: Graph storage (NetworkX/Neo4j)
- BRAIN-032: Graph-based context retrieval

#### Settings & Monitoring (3 stories)
- BRAIN-033: Brain settings UI
- BRAIN-034: Token usage tracker
- BRAIN-035: Cost dashboard

#### Features (5 stories)
- BRAIN-036: Context inspector
- BRAIN-037: Chat with Story
- BRAIN-038: Smart tagging service
- BRAIN-039: Automatic chunking strategy
- BRAIN-040: E2E Brain Test

---

## [2025-02-03] - BRAIN-001 ✅
- Implemented ChromaDB vector store adapter with full CRUD operations
- Created IVectorStore port interface with upsert, query, delete, clear, health_check, count methods
- Added ChromaDB health check endpoint at GET /health/chromadb
- Added register_chromadb_check method to HealthMonitor for system-wide monitoring
- All 23 unit tests passing, including data persistence across restarts test
- Files changed:
  - `pyproject.toml`: Added chromadb>=0.5.0 and tiktoken>=0.7.0 dependencies
  - `src/contexts/knowledge/application/ports/i_vector_store.py`: IVectorStore port, VectorDocument, QueryResult, UpsertResult value objects
  - `src/contexts/knowledge/infrastructure/adapters/chromadb_vector_store.py`: ChromaDBVectorStore adapter
  - `src/api/health_system.py`: Added register_chromadb_check method
  - `src/api/routers/health.py`: Added /health/chromadb endpoint
  - `tests/unit/contexts/knowledge/infrastructure/adapters/test_chromadb_vector_store.py`: Full test coverage

**Learnings for future iterations:**
- ChromaDB uses PersistentClient for embedded mode - no separate server needed
- Default storage is `.data/chroma/` - configurable via CHROMA_PERSIST_DIR env var
- Collection cache must be instance-level (not class-level) to avoid cross-test pollution
- Cosine similarity is the default distance metric for text embeddings
- ChromaDB's `query` returns distances that need to be converted to similarity scores (1 - distance)

---

## [2025-02-03] - BRAIN-002 ✅
- Implemented EmbeddingService adapter with OpenAI embeddings API integration
- Created IEmbeddingService port interface with embed, embed_batch, get_dimension, clear_cache methods
- Added real OpenAI embeddings support (text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002)
- Implemented deterministic fallback mock embeddings for testing
- Added batch embedding support with caching for efficiency
- Maintained backward compatibility with EmbeddingGeneratorAdapter alias
- All 32 unit tests passing
- Files changed:
  - `pyproject.toml`: Added openai>=1.0.0,<2.0.0 dependency
  - `src/contexts/knowledge/application/ports/i_embedding_service.py`: IEmbeddingService port, EmbeddingError exception
  - `src/contexts/knowledge/infrastructure/adapters/embedding_generator_adapter.py`: Complete rewrite of EmbeddingServiceAdapter
  - `tests/unit/contexts/knowledge/infrastructure/adapters/test_embedding_service_adapter.py`: Full test coverage

**Learnings for future iterations:**
- OpenAI's AsyncOpenAI client requires lazy loading to avoid import errors when package isn't installed
- Use TYPE_CHECKING to import types only for mypy, not runtime
- Mock embeddings use text hash for determinism - same text always produces same vector
- Embeddings should be normalized to unit length (standard practice for semantic search)
- Caching at the adapter level avoids redundant API calls for identical text
- Batch mode can be more efficient but should still respect cache for individual items

---

## [2025-02-03] - BRAIN-003 ✅
- Created ChunkingStrategy value object with FIXED/SEMANTIC/SENTENCE/PARAGRAPH strategies
  - Configurable chunk_size (50-2000), overlap, and min_chunk_size
  - Presets for character, scene, and lore content types
- Created SourceType enumeration (CHARACTER, LORE, SCENE, PLOTLINE, ITEM, LOCATION)
- Created SourceKnowledgeEntry entity for RAG with embedding tracking
- Implemented TextChunker domain service with multiple chunking strategies
- Added metadata enrichment with word_count, chunk_index, total_chunks, tags
- All 68 unit tests passing
- Files changed:
  - `src/contexts/knowledge/domain/models/chunking_strategy.py`: ChunkingStrategy VO, ChunkStrategyType enum, ChunkingStrategies factory
  - `src/contexts/knowledge/domain/models/source_type.py`: SourceType enum with from_string()
  - `src/contexts/knowledge/domain/models/source_knowledge_entry.py`: SourceKnowledgeEntry entity, SourceMetadata VO
  - `src/contexts/knowledge/domain/services/text_chunker.py`: TextChunker with FIXED/SENTENCE/PARAGRAPH/SEMANTIC strategies
  - `tests/unit/contexts/knowledge/domain/models/`: Tests for chunking_strategy, source_type, source_knowledge_entry
  - `tests/unit/contexts/knowledge/domain/services/test_text_chunker.py`: 18 tests for all chunking strategies

**Learnings for future iterations:**
- Use `from __future__ import annotations` to enable forward references in enum classmethod return types
- The domain/models directory is gitignored - use `git add -f` to add new model files
- Chunking with overlap means total_words sum includes overlaps (2000 words + overlap = 2200 total_words)
- TextChunker._WORD_PATTERN.findall() is the reliable way to count words (handles irregular spacing)
- For paragraph chunking, exclude delimiter length when calculating text position: end_pos - len(match.group())

---

## Next Story for Ralph
- **BRAIN-007**: Integration: RAG-Enhanced Generation
- Priority: 7
- Focus: Update LLMWorldGenerator to inject retrieved context into prompts
- Dependencies: BRAIN-006 (Context Retrieval Service)
- Dependencies: BRAIN-004 (KnowledgeIngestionService)

---
- Implemented KnowledgeIngestionService with full ingestion pipeline (Text -> Chunk -> Embed -> Store)
- Created `ingest()` method for single entry ingestion with configurable chunking strategy
- Created `batch_ingest()` method for bulk processing with progress tracking via IngestionProgress callback
- Created `delete()` method to remove all chunks for a source with optional source_type filter
- Created `update()` method to replace old chunks with new content (delete + ingest)
- Created `query_by_source()` method to retrieve all chunks for a given source
- Added health_check() and get_count() utility methods
- Implemented value objects: IngestionProgress, IngestionResult, BatchIngestionResult, SourceChunk, RetrievedChunk
- Files changed:
  - `src/contexts/knowledge/application/services/knowledge_ingestion_service.py`: Complete service implementation
  - `src/contexts/knowledge/application/services/__init__.py`: Service module exports
  - `tests/unit/contexts/knowledge/application/services/test_knowledge_ingestion_service.py`: 23 comprehensive tests

**Learnings for future iterations:**
- Use `embed_batch()` for efficiency instead of calling `embed()` in a loop
- The service layer should use `TYPE_CHECKING` for imports that are only needed for type hints
- Progress callbacks can be sync or async - use `inspect.iscoroutinefunction()` to detect
- Mock vector stores need to track state properly for delete operations to be testable
- ChunkingStrategy.from_string() normalizes string inputs to SourceType enum
- VectorDocument metadata is the place to store source tracking info (source_id, source_type, chunk_index)

---

## [2025-02-03] - BRAIN-005 ✅
- Implemented KnowledgeSyncEventHandler with async queue processing for knowledge ingestion
- Created KnowledgeEventSubscriber to bridge domain events (CharacterCreated/Updated, LoreCreated/Updated, SceneCreated/Updated) to ingestion
- Implemented async queue with configurable max_queue_size to avoid blocking
- Added retry logic with exponential backoff (configurable: FIXED/EXPONENTIAL/NONE)
- Implemented dead letter queue for failed tasks with manual retry capability
- Added content formatting helpers: _character_to_content, _lore_to_content, _scene_to_content
- Created IngestionTask value object for tracking retry state
- All 36 unit tests passing (14 for subscriber, 22 for handler)
- Files changed:
  - `src/contexts/knowledge/application/event_handlers/knowledge_sync_event_handler.py`: Main handler with queue, retry, dead letter
  - `src/contexts/knowledge/application/event_handlers/knowledge_event_subscriber.py`: Event subscriber bridging events to ingestion
  - `src/contexts/knowledge/application/event_handlers/__init__.py`: Package exports
  - `tests/unit/contexts/knowledge/application/event_handlers/test_knowledge_sync_event_handler.py`: 22 handler tests
  - `tests/unit/contexts/knowledge/application/event_handlers/test_knowledge_event_subscriber.py`: 14 subscriber tests

**Learnings for future iterations:**
- Async queue processing requires proper event loop management - asyncio.create_task() must be tracked for cleanup
- Use `asyncio.create_task()` with callback to track pending tasks: `task.add_done_callback(lambda t: self._pending_retry_tasks.discard(t))`
- On handler stop, gather all pending retry tasks with `asyncio.gather(*tasks, return_exceptions=True)` for clean shutdown
- asyncio.Queue(0) doesn't work as expected for testing full queue - it's a special synchronous queue
- For testing async handlers with queues, use short delays and queue.join() to wait for processing
- Test dead letter queue by setting max_retries=0 to skip the retry delay
- Content formatting helpers should include all relevant metadata (IDs, categories, tags) for RAG context

---

## [2025-02-04] - BRAIN-006 ✅
- Implemented RetrievalService with full RAG retrieval pipeline (Query -> Embed -> Search -> Filter -> Format)
- Created `retrieve_relevant()` method with configurable k, filters, and options
- Created `format_context()` method that converts chunks to LLM-ready context with token estimation
- Implemented RetrievalFilter for source_type, tags, and date range filtering
- Added RetrievalOptions for relevance threshold (min_score) and deduplication control
- Implemented content-based deduplication using SequenceMatcher for similarity detection
- Created value objects: FormattedContext, RetrievalResult, RetrievalFilter, RetrievalOptions
- All 36 unit tests passing
- Files changed:
  - `src/contexts/knowledge/application/services/retrieval_service.py`: Complete RetrievalService implementation
  - `src/contexts/knowledge/application/services/__init__.py`: Export RetrievalService and related types
  - `tests/unit/contexts/knowledge/application/services/test_retrieval_service.py`: 36 comprehensive tests

**Learnings for future iterations:**
- For token estimation, use ~4 chars per token as a rough estimate (actual tokenization varies by model)
- When deduplicating, sort by score first so highest-scoring chunks are kept when duplicates are found
- Use `set()` for seen_hashes, not `[]` - type hint `set[str] = []` creates a list, not a set
- The `matches()` method should distinguish between `None` metadata (no document) and `{}` metadata (document with no fields)
- ChromaDB's `$in` operator works for OR queries on metadata fields (e.g., source_type in [CHARACTER, LORE])
- SequenceMatcher from difflib provides decent similarity detection for text deduplication
- When fetching k results, request 2x-3x from vector store to account for filtering/deduplication

---

## [2025-02-04] - BRAIN-007A ✅
- Implemented RAGIntegrationService as a high-level wrapper for RAG prompt enrichment
- Created `enrich_prompt(query, base_prompt)` method that retrieves, formats, and injects context
- Added RAGConfig value object for max_chunks, score_threshold, context_token_limit, enabled flags
- Implemented RAGMetrics for tracking queries_total, chunks_retrieved_total, tokens_added_total, failed_queries
- Added factory method `create()` for convenient dependency injection
- Support for RetrievalFilter and per-call config_override
- All 25 unit tests passing
- Files changed:
  - `src/contexts/knowledge/application/services/rag_integration_service.py`: Complete RAGIntegrationService implementation
  - `src/contexts/knowledge/application/services/__init__.py`: Export new service and types
  - `tests/unit/contexts/knowledge/application/services/test_rag_integration_service.py`: 25 comprehensive tests

**Learnings for future iterations:**
- RAGIntegrationService provides a cleaner abstraction than directly using RetrievalService for prompt enrichment
- The service follows a clear pattern: Retrieve -> Format -> Inject -> Return EnrichedPrompt
- Metrics tracking is valuable for debugging RAG performance (avg chunks per query, tokens added)
- Config overrides allow per-request customization without changing global configuration
- When RAG is disabled (enabled=False), the service returns base_prompt unchanged - useful for A/B testing
- The `@dataclass(frozen=True, slots=True)` pattern is ideal for immutable config value objects
- Use `TYPE_CHECKING` for imports only needed in type hints to avoid circular dependencies

---

## [2025-02-04] - BRAIN-007B ✅
- Added optional RAGIntegrationService dependency to LLMWorldGenerator
- Modified `generate_dialogue()` to async with RAG context enrichment before LLM call
- Modified `suggest_next_beats()` to async with RAG context enrichment before LLM call
- Added `_enrich_with_rag()` helper method that calls RAGIntegrationService.enrich_prompt()
- Added `_extract_keywords_for_dialogue()` for building RAG queries from character data
- Added `_extract_keywords_for_beats()` for building RAG queries from beat context
- Added `use_rag` parameter to enable/disable RAG per request (default: True)
- Updated 3 existing tests to async (suggest_next_beats integration tests)
- All 75 unit tests passing (14 new RAG integration tests)
- Files changed:
  - `src/contexts/world/infrastructure/generators/llm_world_generator.py`: Added RAG integration to dialogue and beat suggestion
  - `tests/unit/contexts/world/infrastructure/test_llm_world_generator.py`: Added 14 RAG integration tests

**Learnings for future iterations:**
- When changing a method from sync to async, all calling tests must also be marked async with `@pytest.mark.asyncio`
- Use `TYPE_CHECKING` for type hints of optional dependencies (RAGIntegrationService) to avoid import errors
- The `use_rag` parameter allows per-request control without changing the generator configuration
- Keyword extraction should include character name, top traits, context keywords, and mood direction
- RAG enrichment gracefully falls back to original prompt on error - logged as warning level
- The generator must remain backward compatible (works without RAG service configured)
- When using `await` in tests, ensure the test function is marked with `@pytest.mark.asyncio`

---

## [2025-02-04] - BRAIN-007C ✅
- Updated `_enrich_with_rag()` to return tuple `(prompt, chunks_retrieved, tokens_added)` instead of just prompt
- Modified `generate_dialogue()` to inject RAG context into **system prompt** (not user prompt)
- Modified `suggest_next_beats()` to inject RAG context into **system prompt** (not user prompt)
- Enhanced logging from `debug` to `info` level for RAG enrichment metrics
- Added `rag_context_injected` info log after successful RAG enrichment
- Added 2 new tests: `test_rag_context_injected_into_system_prompt` and `test_rag_context_injection_logs_metrics`
- All 82 unit tests passing (up from 75 - added 7 new tests for BRAIN-007C)
- Files changed:
  - `src/contexts/world/infrastructure/generators/llm_world_generator.py`: Updated RAG context injection to system prompt
  - `tests/unit/contexts/world/infrastructure/test_llm_world_generator.py`: Updated existing tests and added 7 new tests

**Learnings for future iterations:**
- The System Prompt is the correct place to inject RAG context because it provides consistent background knowledge
- The User Prompt should remain clean and focused on the specific request/task
- Returning metrics (chunks, tokens) from `_enrich_with_rag()` enables better observability without additional log calls
- When updating return types of methods, all test assertions must be updated to unpack the new return values
- The `use_rag` parameter already existed from BRAIN-007B - this story focused on proper context placement
- Context injection follows the pattern: "Relevant Context:\n{context}\n\n---\n\n{original_prompt}"
- RAGIntegrationService's `enrich_prompt()` method already handles the "Relevant Context:" header formatting
- Tests that verify internal behavior (like which prompt receives context) require mocking `_call_gemini` to inspect arguments

---

## [2025-02-04] - BRAIN-008A ✅
- Implemented BM25Retriever service for keyword-based search using BM25Plus algorithm
- Added `index_documents()` method for indexing documents with configurable k1 and b parameters
- Added `search(query, k, collection, filters)` method returning BM25Result list sorted by relevance
- Added `remove_document()` and `clear_collection()` methods for index management
- Added `get_stats()` method returning BM25IndexStats with document count, token count, and average doc length
- Implemented `tokenize()` utility function for word-based tokenization with lowercase normalization
- Created value objects: IndexedDocument (frozen), BM25Result (mutable), BM25IndexStats (frozen)
- Added rank-bm25>=0.2.2 dependency to pyproject.toml
- All 51 unit tests passing (including tokenization, indexing, search, filters, and statistics)
- Files changed:
  - `pyproject.toml`: Added rank-bm25>=0.2.2 dependency
  - `src/contexts/knowledge/application/services/bm25_retriever.py`: Complete BM25Retriever implementation (575 lines)
  - `src/contexts/knowledge/application/services/__init__.py`: Export new BM25 types
  - `tests/unit/contexts/knowledge/application/services/test_bm25_retriever.py`: 51 comprehensive tests

**Learnings for future iterations:**
- BM25Plus works better than BM25Okapi for small corpora because it adds a delta parameter that prevents zero IDF scores
- With BM25, when a term appears in exactly half the documents (df = N/2), IDF becomes log(1) = 0, resulting in zero scores
- BM25 scores can be negative for short documents or rare terms - this is normal behavior due to length normalization
- The `k1` parameter controls term saturation (1.0-2.0 typical): higher = more weight to term frequency
- The `b` parameter controls length normalization (0.0-1.0): higher = more penalty for longer documents
- For keyword search, tokens should be pre-computed and stored to avoid re-tokenizing on every search
- Use `type: ignore[import-untyped]` for libraries without type stubs like rank-bm25
- When using argsort on numpy arrays, reverse with `[::-1]` to get highest scores first

---

## [2025-02-04] - BRAIN-008B ✅
- Implemented HybridRetriever service combining vector (semantic) and BM25 (keyword) search
- Created HybridConfig value object with configurable weights and RRF parameters
- Implemented Reciprocal Rank Fusion (RRF) with configurable k constant (default 60)
- Implemented Linear Score Fusion for pure score-based combination
- Implemented Hybrid Score Fusion combining RRF and linear with alpha parameter (0.0-1.0)
- Added support for per-search config_override to customize fusion behavior
- Graceful degradation: if one search method fails, returns results from the other
- Content-based deduplication using MD5 hashing
- All 44 unit tests passing (covering normalization, RRF, linear fusion, hybrid fusion, config, retriever)
- Files changed:
  - `src/contexts/knowledge/application/services/hybrid_retriever.py`: Complete HybridRetriever implementation (730 lines)
  - `src/contexts/knowledge/application/services/__init__.py`: Export new HybridRetriever types
  - `tests/unit/contexts/knowledge/application/services/test_hybrid_retriever.py`: 44 comprehensive tests

**Learnings for future iterations:**
- Hybrid retrieval provides the best of both worlds: semantic understanding from vector search + exact keyword matching from BM25
- Reciprocal Rank Fusion (RRF) is robust to score scale differences because it operates on ranks, not raw scores
- The RRF k parameter controls the influence of rank (default 60): smaller k = top ranks dominate, larger k = more influence from lower ranks
- Linear fusion requires careful normalization because vector scores (0-1) and BM25 scores (unbounded) have different scales
- The rrf_alpha parameter allows blending between pure rank-based (1.0) and pure score-based (0.0) fusion
- When using TYPE_CHECKING for imports, make sure to use correct relative import depth (e.g., `...domain.models` not `.domain.models`)
- For frozen dataclasses, `__post_init__` must have `-> None` return type annotation to satisfy mypy
- The fusion method naming: "linear" (use_rrf=False or rrf_alpha=0), "rrf" (rrf_alpha=1.0), "hybrid" (0 < rrf_alpha < 1.0)
- Deduplication should use content hash (MD5) to detect identical chunks from both sources
- When ranking results from different sources, store both score and rank for fusion algorithms

---

## [2025-02-04] - BRAIN-009A ✅
- Implemented ILLMClient port interface for LLM abstraction
- Implemented GeminiLLMClient adapter with Gemini API integration
- Implemented QueryRewriter service with three strategies:
  * SYNONYM: Expand queries with synonyms and related terms
  * DECOMPOSE: Break complex queries into sub-queries
  * HYBRID: Combine both approaches
- Added caching for rewritten queries to reduce token usage
- Added graceful degradation on LLM errors (returns original query)
- Added MockLLMClient for testing without API calls
- All 49 unit tests passing
- Files changed:
  - `src/contexts/knowledge/application/ports/i_llm_client.py`: ILLMClient port, LLMRequest, LLMResponse value objects
  - `src/contexts/knowledge/infrastructure/adapters/gemini_llm_client.py`: GeminiLLMClient and MockLLMClient
  - `src/contexts/knowledge/application/services/query_rewriter.py`: Complete QueryRewriter implementation (550 lines)
  - `src/contexts/knowledge/application/services/__init__.py`: Export new types
  - `tests/unit/contexts/knowledge/application/services/test_query_rewriter.py`: 49 comprehensive tests

**Learnings for future iterations:**
- The ILLMClient port interface provides abstraction for different LLM providers (OpenAI, Anthropic, Gemini, etc.)
- When creating async services, always mark test methods with @pytest.mark.asyncio
- RewriteStrategy as str, Enum works for string-compatible enums (strategy.value == "synonym")
- Query rewriting uses lower temperature (0.3) for more deterministic rewrites
- Cache keys should be strategy-specific and case-insensitive for better hit rates
- Graceful degradation: on LLM error, return original query rather than failing
- JSON parsing from LLM responses needs multiple fallback strategies (direct parse, markdown extraction, line splitting)

---