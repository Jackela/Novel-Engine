# Ralph Progress Log - Warzone 4: AI Brain
Started: 2025-02-05 (Reset)

Campaign: WARZONE 4 - Building the RAG (Retrieval-Augmented Generation) engine
Branch: feat/code-citadel

---

## Codebase Patterns

### Architecture
- **Hexagonal Architecture**: Routers -> Services -> Domain. No business logic in routers.
- **Result Pattern**: Use `Result[T, E]` for error handling in services (see `src/core/result.py`)
- **Domain Events**: Use events for cross-context communication, not direct imports
- **Import Linting**: Rules defined in `.importlinter` to enforce architectural boundaries
- **Vector Storage**: ChromaDB adapter in `src/contexts/knowledge/infrastructure/adapters/chromadb_vector_store.py`
- **Type Annotations**: Use `from __future__ import annotations` and import List/Tuple from typing for complex types
- **RAG Retrieval**: RetrievalService in `src/contexts/knowledge/application/services/retrieval_service.py` with filtering and deduplication
- **BM25 Keyword Search**: BM25Retriever in `src/contexts/knowledge/application/services/bm25_retriever.py` for exact keyword matching
- **Hybrid Retrieval**: HybridRetriever in `src/contexts/knowledge/application/services/hybrid_retriever.py` combines vector and BM25 with RRF
- **Query Rewriting**: QueryRewriter in `src/contexts/knowledge/application/services/query_rewriter.py` with SYNONYM/DECOMPOSE/CLARIFICATION strategies
- **Reranking**: IReranker port in `src/contexts/knowledge/application/ports/i_reranker.py` and RerankService with fallback behavior
- **Token Counting**: TokenCounter in `src/contexts/knowledge/application/services/token_counter.py` with multi-provider tiktoken support
- **Context Optimization**: ContextOptimizer in `src/contexts/knowledge/application/services/context_optimizer.py` with 4 packing strategies (relevance, diversity, remove_redundancy, compress_summaries)
- **Prompt Templates**: PromptTemplate entity in `src/contexts/knowledge/domain/models/prompt_template.py` with {{variable}} syntax and version tracking
- **Prompt Inheritance**: Use `extends` field and `{{> template}}` syntax for template composition. Variable override works by child having same-named variable.
- **Prompt Repository**: IPromptRepository port in `src/contexts/knowledge/application/ports/i_prompt_repository.py` with CRUD, version history, and search
- **Model Registry**: ModelRegistry in `src/contexts/knowledge/application/services/model_registry.py` with task-based routing and aliases
- **Entity Extraction**: EntityExtractionService in `src/contexts/knowledge/application/services/entity_extraction_service.py` with LLM-based extraction
- **Co-reference Resolution**: CoreferenceResolutionService in `src/contexts/knowledge/application/services/coreference_resolution_service.py` with heuristic + LLM fallback
- **Large Text Processing**: extract_large_text() with chunking (chunk_size, overlap) and merging for texts exceeding token limits
- **Token Tracking**: TokenTracker in `src/contexts/knowledge/application/services/token_tracker.py` with decorator/context manager support
- **Token Usage Entity**: TokenUsage in `src/contexts/knowledge/domain/models/token_usage.py` with cost calculation using model pricing
- **Token Usage Repository**: ITokenUsageRepository port in `src/contexts/knowledge/application/ports/i_token_usage_repository.py` with InMemoryTokenUsageRepository implementation
- **Budget Alerts**: BudgetAlertService in `src/contexts/knowledge/application/services/budget_alert_service.py` with threshold-based alerting
- **Alert Configuration**: BudgetAlertConfig in `src/contexts/knowledge/domain/models/budget_alert.py` with threshold types, operators, severity, frequency
- **Real-time Broadcasting**: RealtimeUsageBroadcaster in `src/api/routers/brain_settings.py` provides SSE-based live token updates to frontend clients
- **Hexagonal Architecture**: Routers -> Services -> Domain. No business logic in routers.
- **Result Pattern**: Use `Result[T, E]` for error handling in services (see `src/core/result.py`)
- **Domain Events**: Use events for cross-context communication, not direct imports
- **Import Linting**: Rules defined in `.importlinter` to enforce architectural boundaries
- **Vector Storage**: ChromaDB adapter in `src/contexts/knowledge/infrastructure/adapters/chromadb_vector_store.py`
- **Type Annotations**: Use `from __future__ import annotations` and import List/Tuple from typing for complex types
- **RAG Retrieval**: RetrievalService in `src/contexts/knowledge/application/services/retrieval_service.py` with filtering and deduplication
- **BM25 Keyword Search**: BM25Retriever in `src/contexts/knowledge/application/services/bm25_retriever.py` for exact keyword matching
- **Hybrid Retrieval**: HybridRetriever in `src/contexts/knowledge/application/services/hybrid_retriever.py` combines vector and BM25 with RRF
- **Query Rewriting**: QueryRewriter in `src/contexts/knowledge/application/services/query_rewriter.py` with SYNONYM/DECOMPOSE/CLARIFICATION strategies
- **Reranking**: IReranker port in `src/contexts/knowledge/application/ports/i_reranker.py` and RerankService with fallback behavior
- **Token Counting**: TokenCounter in `src/contexts/knowledge/application/services/token_counter.py` with multi-provider tiktoken support
- **Context Optimization**: ContextOptimizer in `src/contexts/knowledge/application/services/context_optimizer.py` with 4 packing strategies (relevance, diversity, remove_redundancy, compress_summaries)
- **Prompt Templates**: PromptTemplate entity in `src/contexts/knowledge/domain/models/prompt_template.py` with {{variable}} syntax and version tracking
- **Prompt Inheritance**: Use `extends` field and `{{> template}}` syntax for template composition. Variable override works by child having same-named variable.
- **Prompt Repository**: IPromptRepository port in `src/contexts/knowledge/application/ports/i_prompt_repository.py` with CRUD, version history, and search
- **Model Registry**: ModelRegistry in `src/contexts/knowledge/application/services/model_registry.py` with task-based routing and aliases
- **Entity Extraction**: EntityExtractionService in `src/contexts/knowledge/application/services/entity_extraction_service.py` with LLM-based extraction
- **Co-reference Resolution**: CoreferenceResolutionService in `src/contexts/knowledge/application/services/coreference_resolution_service.py` with heuristic + LLM fallback
- **Large Text Processing**: extract_large_text() with chunking (chunk_size, overlap) and merging for texts exceeding token limits
- **Token Tracking**: TokenTracker in `src/contexts/knowledge/application/services/token_tracker.py` with decorator/context manager support
- **Token Usage Entity**: TokenUsage in `src/contexts/knowledge/domain/models/token_usage.py` with cost calculation using model pricing
- **Token Usage Repository**: ITokenUsageRepository port in `src/contexts/knowledge/application/ports/i_token_usage_repository.py` with InMemoryTokenUsageRepository implementation
- **Budget Alerts**: BudgetAlertService in `src/contexts/knowledge/application/services/budget_alert_service.py` with threshold-based alerting
- **Alert Configuration**: BudgetAlertConfig in `src/contexts/knowledge/domain/models/budget_alert.py` with threshold types, operators, severity, frequency

### Frontend
- **Zustand Stores**: Feature-based organization (4 stores: authStore, orchestrationStore, decisionStore, weaverStore)
- **State Management Hierarchy**: TanStack Query -> Zustand -> React Hook Form -> useState
- **Schema SSOT**: Backend Pydantic schemas (`src/api/schemas.py`) drive frontend Zod schemas (`frontend/src/types/schemas.ts`)

### Testing
- **TDD/BDD**: Write failing test first, implement minimum code, verify with E2E
- **Test File Organization**: Keep files under 500 lines, split by functionality
- **Quality Gates**: `pytest tests/ && npm run test:e2e && mypy . && npm run type-check`

### Schema Synchronization
- Backend schemas are SSOT in `src/api/schemas.py`
- Frontend schemas live in `frontend/src/types/schemas.ts` (Zod)
- Regenerate OpenAPI with `python scripts/generate_openapi.py` after schema changes
- Zod's `z.number()` is correct for both Python `int` and `float`

---

## 2025-02-05 - BRAIN-035B-01
- **What was implemented:**
  - Added GET `/api/brain/models` endpoint returning model pricing data from ModelRegistry
  - Added `ModelPricingResponse` Pydantic model with provider, model name, display name, costs, and context info
  - Added frontend `getModelPricing()` API function
  - Created model comparison table in Usage tab of Brain Settings page
  - Table displays models grouped by provider (OpenAI, Anthropic, Gemini, Ollama)
  - Shows cost per 1M tokens (input/output) and context window size
- **Files changed:**
  - `src/api/routers/brain_settings.py` - Added model pricing endpoint
  - `frontend/src/features/routing/api/brainSettingsApi.ts` - Added API function
  - `frontend/src/features/routing/components/BrainSettingsPage.tsx` - Added comparison table UI
- **Learnings for future iterations:**
  - ModelRegistry DEFAULT_MODELS dict contains all model definitions with pricing
  - Use `provider` value for API responses, format for display in UI
  - Brain Settings Usage tab is the right place for pricing comparison
  - Table component from shadcn/ui provides consistent styling

---

## 2025-02-05 - BRAIN-035B-02
- **What was implemented:**
  - Added preset/custom mode toggle for date range filtering in Usage tab
  - Preset mode: 7, 30, 90 day quick-select buttons (existing functionality enhanced)
  - Custom mode: Native HTML date inputs for start/end date selection
  - Apply button calculates days from custom date range
  - Filter applies to all Usage tab charts (Tokens Over Time, Cost by Model, Provider Distribution)
  - Defaults to 30 days in preset mode
- **Files changed:**
  - `frontend/src/features/routing/components/BrainSettingsPage.tsx` - Added date range picker UI
- **Learnings for future iterations:**
  - Use `effectiveDays` pattern to calculate days from both preset and custom date ranges
  - Native HTML date inputs work well and don't require additional dependencies
  - Mode toggle (preset vs custom) provides good UX flexibility

---

## 2025-02-05 - BRAIN-035B-03
- **What was implemented:**
  - Added GET `/api/brain/usage/export` endpoint returning CSV file with usage data
  - CSV columns: Timestamp, Provider, Model, Input/Output/Total Tokens, Costs, Latency, Success
  - Added `exportUsageCsv()` frontend API function with file download handling
  - Added Export CSV button with Download icon to Usage tab header
  - Export respects current date range filter (uses `effectiveDays`)
- **Files changed:**
  - `src/api/routers/brain_settings.py` - Added CSV export endpoint
  - `frontend/src/features/routing/api/brainSettingsApi.ts` - Added export function
  - `frontend/src/features/routing/components/BrainSettingsPage.tsx` - Added export button
- **Learnings for future iterations:**
  - Use FastAPI Response with media_type "text/csv" for CSV downloads
  - Set Content-Disposition header with filename for proper browser download behavior
  - Frontend: Use blob download pattern with createObjectURL for file downloads

---

## 2025-02-05 - BRAIN-035B-04
- **What was implemented:**
  - Added RealtimeUsageBroadcaster class for SSE-based real-time token usage broadcasting
  - Added GET `/api/brain/usage/stream` SSE endpoint for live token updates
  - Added frontend types for real-time usage events (session_start, token_update, session_complete, session_state, error)
  - Added `useRealtimeUsage` React hook for subscribing to SSE events
  - Added RealtimeUsageCounter component that displays live token usage during generation
  - Counter shows active sessions with tokens, cost, model info, and connection status
  - Auto-removes completed sessions after 5 seconds from UI
- **Files changed:**
  - `src/api/routers/brain_settings.py` - Added RealtimeUsageBroadcaster and SSE endpoint
  - `frontend/src/features/routing/api/brainSettingsApi.ts` - Added event types and streamRealtimeUsage function
  - `frontend/src/features/routing/components/BrainSettingsPage.tsx` - Added useRealtimeUsage hook and RealtimeUsageCounter component
- **Learnings for future iterations:**
  - SSE pattern: Use `yield f"data: {json}\n\n"` format for Server-Sent Events
  - Frontend: EventSource auto-reconnects on disconnect - handle cleanup in useEffect return
  - Global broadcaster pattern allows fan-out to multiple connected clients
  - Session-based tracking isolates concurrent user generations
  - Real-time counter appears at top of Usage tab when active sessions exist

---

## 2025-02-05 - BRAIN-034A
- **What was implemented:**
  - Added ITokenUsageRepository port with filtering, aggregation, and persistence methods
  - Added InMemoryTokenUsageRepository adapter for development/testing
  - Added TokenTracker service with decorator and context manager support
  - Added input_tokens, output_tokens, raw_usage fields to LLMResponse
  - Updated Claude, Gemini, Ollama, and OpenAI LLM clients to populate new token fields
  - Added comprehensive unit tests for TokenTracker
- **Files changed:**
  - `src/contexts/knowledge/application/ports/i_llm_client.py` - Added input_tokens, output_tokens, raw_usage to LLMResponse
  - `src/contexts/knowledge/application/ports/i_token_usage_repository.py` - New port for token usage persistence
  - `src/contexts/knowledge/application/services/token_tracker.py` - New service with decorator/context manager
  - `src/contexts/knowledge/infrastructure/adapters/__init__.py` - Exported new repository
  - `src/contexts/knowledge/infrastructure/adapters/in_memory_token_usage_repository.py` - New adapter
  - `src/contexts/knowledge/infrastructure/adapters/claude_llm_client.py` - Populate token fields
  - `src/contexts/knowledge/infrastructure/adapters/gemini_llm_client.py` - Populate token fields
  - `src/contexts/knowledge/infrastructure/adapters/ollama_llm_client.py` - Populate token fields
  - `src/contexts/knowledge/infrastructure/adapters/openai_llm_client.py` - Populate token fields
  - `tests/unit/contexts/knowledge/application/services/test_token_tracker.py` - New test file
- **Learnings for future iterations:**
  - Use regular class instead of dataclass when defining custom __init__ with keyword-only arguments
  - Move runtime type imports (like LLMResponse) out of TYPE_CHECKING block if used for isinstance checks
  - TokenTracker decorator tracks LLM calls automatically via @tracker.track_llm_call(model_ref="...")
  - Context manager pattern: async with tracker.track_call(model, prompt=...) as ctx: ctx.record_success(...)
  - All 16 tests pass for TokenTracker functionality

---

## 2025-02-05 - BRAIN-036-01
- **What was implemented:**
  - Added View AI Context button to scene editor toolbar
  - Added ragEnabled and onViewAIContext props to EditorComponent
  - Button uses Brain icon from lucide-react
  - Button only visible when RAG is enabled (ragEnabled prop)
  - Button is disabled during streaming generation
  - Follows existing shadcn/ui Button patterns with outline variant
- **Files changed:**
  - `frontend/src/components/narrative/EditorComponent.tsx` - Added View AI Context button
- **Learnings for future iterations:**
  - EditorToolbar is nested function inside EditorComponent (lines 84-184)
  - Brain icon from lucide-react fits the AI theme
  - Use spread operator with conditional check for optional props: `{...(prop !== undefined && { prop })}`
  - EditorComponent receives sceneId, prompt, context props from parent
  - Parent component needs to pass ragEnabled from brainSettings.rag_config.enabled

---

## 2025-02-05 - BRAIN-036-02
- **What was implemented:**
  - Created ContextInspector sliding panel component using shadcn/ui Sheet
  - Added GET `/api/brain/context` endpoint returning retrieved RAG chunks with metadata
  - Added RetrievedChunkResponse and RAGContextResponse schemas
  - Panel shows retrieved chunks with source info (type, ID, content)
  - Displays relevance scores with color coding (green >= 0.8, yellow >= 0.6, orange < 0.6)
  - Shows token count per chunk
  - Integrated with NarrativeEditorLayout with state management
  - Uses scene content as query for context retrieval
- **Files changed:**
  - `src/api/schemas.py` - Added RAGContextResponse, RetrievedChunkResponse
  - `src/api/routers/brain_settings.py` - Added get_rag_context endpoint
  - `frontend/src/features/routing/api/brainSettingsApi.ts` - Added getRAGContext API function and types
  - `frontend/src/components/narrative/ContextInspector.tsx` - New component
  - `frontend/src/components/narrative/NarrativeEditorLayout.tsx` - Integrated ContextInspector
- **Learnings for future iterations:**
  - TokenCounter.count(text) returns TokenCountResult, access token_count via result.token_count
  - Sheet component from shadcn/ui provides sliding panel behavior (side="right" default)
  - Use exactOptionalPropertyTypes: true requires handling undefined props carefully with spread operator
  - RetrievalService.get_sources() returns list of source dicts with source_type, source_id, display_name, etc.
  - Source type badges use color mapping for visual distinction (CHARACTER=purple, LORE=blue, etc.)

---

## 2025-02-05 - BRAIN-036-03
- **What was implemented:**
  - Added `used` boolean field to RetrievedChunkResponse Pydantic model
  - Added `used_threshold` query parameter to GET `/api/brain/context` endpoint (default: 0.7)
  - Chunks with score >= used_threshold are marked as "used" in the response
  - Updated frontend RetrievedChunkResponse TypeScript interface with `used` field
  - Added visual highlighting for used chunks in ContextInspector:
    - Green border and background tint (bg-green-50/50, border-green-200)
    - "Used" badge with CheckCircle2 icon
    - Dark mode support (bg-green-950/30, border-green-800)
- **Files changed:**
  - `src/api/schemas.py` - Added `used` field to RetrievedChunkResponse
  - `src/api/routers/brain_settings.py` - Added used_threshold parameter and logic
  - `frontend/src/features/routing/api/brainSettingsApi.ts` - Added `used` field to interface
  - `frontend/src/components/narrative/ContextInspector.tsx` - Added visual highlighting
- **Learnings for future iterations:**
  - Used chunks are determined by relevance threshold (0.7 by default)
  - Visual distinction uses conditional classes based on `chunk.used` boolean
  - CheckCircle2 icon from lucide-react for used indicator badge
  - Green color scheme indicates "used" state (consistent with success/positive semantics)

---

## 2025-02-05 - BRAIN-036-04
- **What was implemented:**
  - Added Checkbox component from shadcn/ui for chunk selection
  - Added checkboxes to each chunk card in ContextInspector
  - Added Select All / Deselect All buttons in header
  - Added Regenerate button in footer that appears when chunks are selected
  - Added visual ring styling (ring-2 ring-primary) for selected chunks
  - Added selection count indicator in header description
  - Added optional onRegenerateWithChunks callback prop for future integration
  - Chunks are deselected when fetching new context
- **Files changed:**
  - `frontend/src/components/ui/checkbox.tsx` - New component from shadcn/ui
  - `frontend/src/components/narrative/ContextInspector.tsx` - Added checkboxes, selection state, regenerate button
  - `frontend/src/components/narrative/NarrativeEditorLayout.tsx` - Added onRegenerateWithChunks prop
- **Learnings for future iterations:**
  - Use @radix-ui/react-checkbox for accessible checkbox implementation
  - Set<string> is efficient for tracking selected IDs with O(1) lookup
  - Ring styling (ring-2 ring-primary ring-offset-2) provides clear visual feedback for selection
  - Toast notifications provide feedback for regenerate action
  - The actual scene generation integration with selected chunks is a future enhancement

---

## 2025-02-05 - BRAIN-036-05
- **What was implemented:**
  - Added Progress component from shadcn/ui for context window display
  - Added context window usage display between header and content
  - Shows used tokens / limit with percentage
  - Progress bar with color coding (default, orange at 80%, red at 95%)
  - Warning message with AlertTriangle icon when approaching limit (80%+)
  - Different warning messages for near limit vs nearly full
  - Added contextTokenLimit prop (defaults to 4000)
- **Files changed:**
  - `frontend/src/components/ui/progress.tsx` - New component from shadcn/ui
  - `frontend/src/components/narrative/ContextInspector.tsx` - Added context window display
- **Learnings for future iterations:**
  - Use @radix-ui/react-progress for accessible progress bar
  - Threshold-based color coding helps users understand urgency
  - The default context token limit of 4000 is typical for many LLMs
  - Using Tailwind's `[&>div]:bg-*` syntax allows targeting ProgressPrimitive.Indicator

---

## 2025-02-05 - BRAIN-037A-01
- **What was implemented:**
  - Added POST /api/brain/chat endpoint
  - Created ChatMessage, ChatRequest, ChatChunk Pydantic models
  - Endpoint accepts query parameter and optional chat_history
  - Returns SSE (Server-Sent Events) streaming response
  - Integrates with RAG when enabled (RetrievalService for context)
  - Mock streaming response with word-by-word chunks for now
  - Full LLM integration will be in future story (BRAIN-037A-02)
- **Files changed:**
  - `src/api/routers/brain_settings.py` - Added chat endpoint and models
- **Learnings for future iterations:**
  - Use AsyncIterator for SSE streaming responses in FastAPI
  - ChatMessage has role ("user"/"assistant") and content fields
  - ChatRequest has query, chat_history (optional), scene_id (optional), max_chunks
  - ChatChunk has delta (text content) and done (boolean) fields
  - StreamingResponse requires media_type="text/event-stream" for SSE
  - Set Cache-Control: no-cache, Connection: keep-alive, X-Accel-Buffering: no headers
  - Mock response word-by-word with asyncio.sleep(0.02) simulates LLM streaming

---

## 2025-02-05 - BRAIN-037A-02
- **What was implemented:**
  - RAG Integration was already implemented as part of BRAIN-037A-01
  - The chat endpoint calls RetrievalService with user query
  - Retrieved context is injected into the system prompt
  - Chat responses include formatted context from knowledge base
  - Shows number of relevant chunks found in response
- **Files changed:**
  - None (already implemented in BRAIN-037A-01)
- **Learnings for future iterations:**
  - RAG integration was done alongside the POST endpoint creation
  - RetrievalService.retrieve_relevant() is called with query and k chunks
  - Context is formatted as "[Source N: type:id]" followed by content
  - The system prompt is built with context when RAG is enabled

---
