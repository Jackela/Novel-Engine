# Ralph Progress Log - Warzone 4: AI Brain
Started: 2025-02-03

Campaign: WARZONE 4 - Building the RAG (Retrieval-Augmented Generation) engine
Branch: feat/code-citadel
Stories: 40 (BRAIN-001 through BRAIN-040)

---

## Codebase Patterns

### Architecture
- **Hexagonal Architecture**: Routers -> Services -> Domain. No business logic in routers.
- **Result Pattern**: Use `Result[T, E]` for error handling in services (see `src/core/result.py`)
- **Domain Events**: Use events for cross-context communication, not direct imports
- **Import Linting**: Rules defined in `.importlinter` to enforce architectural boundaries
- **Vector Storage**: ChromaDB adapter in `src/contexts/knowledge/infrastructure/adapters/chromadb_vector_store.py`
- **Type Annotations**: Use `from __future__ import annotations` and import List/Tuple from typing for complex types
- **RAG Retrieval**: RetrievalService in `src/contexts/knowledge/application/services/retrieval_service.py` with filtering and deduplication
- **BM25 Keyword Search**: BM25Retriever in `src/contexts/knowledge/application/services/bm25_retriever.py` for exact keyword matching
- **Hybrid Retrieval**: HybridRetriever in `src/contexts/knowledge/application/services/hybrid_retriever.py` combines vector and BM25 with RRF
- **Query Rewriting**: QueryRewriter in `src/contexts/knowledge/application/services/query_rewriter.py` with SYNONYM/DECOMPOSE/CLARIFICATION strategies
- **Reranking**: IReranker port in `src/contexts/knowledge/application/ports/i_reranker.py` and RerankService with fallback behavior
- **Token Counting**: TokenCounter in `src/contexts/knowledge/application/services/token_counter.py` with multi-provider tiktoken support
- **Context Optimization**: ContextOptimizer in `src/contexts/knowledge/application/services/context_optimizer.py` with 4 packing strategies (relevance, diversity, remove_redundancy, compress_summaries)
- **Prompt Templates**: PromptTemplate entity in `src/contexts/knowledge/domain/models/prompt_template.py` with {{variable}} syntax and version tracking
- **Prompt Inheritance**: Use `extends` field and `{{> template}}` syntax for template composition. Variable override works by child having same-named variable.
- **Prompt Repository**: IPromptRepository port in `src/contexts/knowledge/application/ports/i_prompt_repository.py` with CRUD, version history, and search

### Frontend
- **Zustand Stores**: Feature-based organization (4 stores: authStore, orchestrationStore, decisionStore, weaverStore)
- **State Management Hierarchy**: TanStack Query -> Zustand -> React Hook Form -> useState
- **Schema SSOT**: Backend Pydantic schemas (`src/api/schemas.py`) drive frontend Zod schemas (`frontend/src/types/schemas.ts`)

### Testing
- **TDD/BDD**: Write failing test first, implement minimum code, verify with E2E
- **Test File Organization**: Keep files under 500 lines, split by functionality
- **Quality Gates**: `pytest tests/ && npm run test:e2e && mypy . && npm run type-check`

### Schema Synchronization
- Backend schemas are SSOT in `src/api/schemas.py`
- Frontend schemas live in `frontend/src/types/schemas.ts` (Zod)
- Regenerate OpenAPI with `python scripts/generate_openapi.py` after schema changes
- Zod's `z.number()` is correct for both Python `int` and `float`

---

## Warzone 4: AI Brain - Campaign Overview

### Goal
Build a RAG (Retrieval-Augmented Generation) engine and Prompt Engineering Lab to give the AI system "long-term memory."

### Key Technologies
- **Vector Database**: ChromaDB for semantic search
- **Embeddings**: OpenAI `text-embedding-3-small` (1536 dimensions)
- **LLM Providers**: OpenAI, Anthropic Claude, Google Gemini, Ollama (local)
- **Knowledge Graph**: NetworkX/Neo4j for entity relationships

### Story Breakdown

#### Core Infrastructure (7 stories)
- BRAIN-001: ChromaDB setup ✅
- BRAIN-002: Embedding service ✅
- BRAIN-003: Knowledge entry entity ✅
- BRAIN-004: Knowledge ingestion service ✅
- BRAIN-005: Auto-sync event listeners ✅
- BRAIN-006: Context retrieval service (RAG) ✅
- BRAIN-007: RAG-enhanced generation ✅

#### RAG Enhancement (6 stories)
- BRAIN-008A: Hybrid Search - BM25 Implementation ✅
- BRAIN-008B: Hybrid Search - Score Fusion ✅
- BRAIN-009: Query rewriting & expansion ✅
- BRAIN-010: Re-ranking service ✅
- BRAIN-011: Context window optimization ✅
- BRAIN-012: Citation & source attribution
- BRAIN-013: Multi-hop reasoning

#### Prompt Engineering (9 stories)
- BRAIN-014: Prompt template entity
- BRAIN-015: Prompt management API
- BRAIN-016: Version control & history
- BRAIN-017: Template inheritance & composition
- BRAIN-018: A/B testing framework
- BRAIN-019: Prompt Engineering Lab UI
- BRAIN-020: Prompt playground
- BRAIN-021: Prompt comparison view
- BRAIN-022: Prompt analytics

#### LLM Integration (6 stories)
- BRAIN-023: Model registry
- BRAIN-024: Anthropic Claude integration
- BRAIN-025: Google Gemini integration
- BRAIN-026: OpenAI GPT-4 Turbo integration
- BRAIN-027: Local LLM support (Ollama)
- BRAIN-028: Model routing strategy

#### Knowledge Graph (4 stories)
- BRAIN-029: Entity extraction service
- BRAIN-030: Relationship extraction
- BRAIN-031: Graph storage (NetworkX/Neo4j)
- BRAIN-032: Graph-based context retrieval

#### Settings & Monitoring (3 stories)
- BRAIN-033: Brain settings UI
- BRAIN-034: Token usage tracker
- BRAIN-035: Cost dashboard

#### Features (5 stories)
- BRAIN-036: Context inspector
- BRAIN-037: Chat with Story
- BRAIN-038: Smart tagging service
- BRAIN-039: Automatic chunking strategy
- BRAIN-040: E2E Brain Test

---

## [2025-02-03] - BRAIN-001 ✅
- Implemented ChromaDB vector store adapter with full CRUD operations
- Created IVectorStore port interface with upsert, query, delete, clear, health_check, count methods
- Added ChromaDB health check endpoint at GET /health/chromadb
- Added register_chromadb_check method to HealthMonitor for system-wide monitoring
- All 23 unit tests passing, including data persistence across restarts test
- Files changed:
  - `pyproject.toml`: Added chromadb>=0.5.0 and tiktoken>=0.7.0 dependencies
  - `src/contexts/knowledge/application/ports/i_vector_store.py`: IVectorStore port, VectorDocument, QueryResult, UpsertResult value objects
  - `src/contexts/knowledge/infrastructure/adapters/chromadb_vector_store.py`: ChromaDBVectorStore adapter
  - `src/api/health_system.py`: Added register_chromadb_check method
  - `src/api/routers/health.py`: Added /health/chromadb endpoint
  - `tests/unit/contexts/knowledge/infrastructure/adapters/test_chromadb_vector_store.py`: Full test coverage

**Learnings for future iterations:**
- ChromaDB uses PersistentClient for embedded mode - no separate server needed
- Default storage is `.data/chroma/` - configurable via CHROMA_PERSIST_DIR env var
- Collection cache must be instance-level (not class-level) to avoid cross-test pollution
- Cosine similarity is the default distance metric for text embeddings
- ChromaDB's `query` returns distances that need to be converted to similarity scores (1 - distance)

---

## [2025-02-03] - BRAIN-002 ✅
- Implemented EmbeddingService adapter with OpenAI embeddings API integration
- Created IEmbeddingService port interface with embed, embed_batch, get_dimension, clear_cache methods
- Added real OpenAI embeddings support (text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002)
- Implemented deterministic fallback mock embeddings for testing
- Added batch embedding support with caching for efficiency
- Maintained backward compatibility with EmbeddingGeneratorAdapter alias
- All 32 unit tests passing
- Files changed:
  - `pyproject.toml`: Added openai>=1.0.0,<2.0.0 dependency
  - `src/contexts/knowledge/application/ports/i_embedding_service.py`: IEmbeddingService port, EmbeddingError exception
  - `src/contexts/knowledge/infrastructure/adapters/embedding_generator_adapter.py`: Complete rewrite of EmbeddingServiceAdapter
  - `tests/unit/contexts/knowledge/infrastructure/adapters/test_embedding_service_adapter.py`: Full test coverage

**Learnings for future iterations:**
- OpenAI's AsyncOpenAI client requires lazy loading to avoid import errors when package isn't installed
- Use TYPE_CHECKING to import types only for mypy, not runtime
- Mock embeddings use text hash for determinism - same text always produces same vector
- Embeddings should be normalized to unit length (standard practice for semantic search)
- Caching at the adapter level avoids redundant API calls for identical text
- Batch mode can be more efficient but should still respect cache for individual items

---

## [2025-02-03] - BRAIN-003 ✅
- Created ChunkingStrategy value object with FIXED/SEMANTIC/SENTENCE/PARAGRAPH strategies
  - Configurable chunk_size (50-2000), overlap, and min_chunk_size
  - Presets for character, scene, and lore content types
- Created SourceType enumeration (CHARACTER, LORE, SCENE, PLOTLINE, ITEM, LOCATION)
- Created SourceKnowledgeEntry entity for RAG with embedding tracking
- Implemented TextChunker domain service with multiple chunking strategies
- Added metadata enrichment with word_count, chunk_index, total_chunks, tags
- All 68 unit tests passing
- Files changed:
  - `src/contexts/knowledge/domain/models/chunking_strategy.py`: ChunkingStrategy VO, ChunkStrategyType enum, ChunkingStrategies factory
  - `src/contexts/knowledge/domain/models/source_type.py`: SourceType enum with from_string()
  - `src/contexts/knowledge/domain/models/source_knowledge_entry.py`: SourceKnowledgeEntry entity, SourceMetadata VO
  - `src/contexts/knowledge/domain/services/text_chunker.py`: TextChunker with FIXED/SENTENCE/PARAGRAPH/SEMANTIC strategies
  - `tests/unit/contexts/knowledge/domain/models/`: Tests for chunking_strategy, source_type, source_knowledge_entry
  - `tests/unit/contexts/knowledge/domain/services/test_text_chunker.py`: 18 tests for all chunking strategies

**Learnings for future iterations:**
- Use `from __future__ import annotations` to enable forward references in enum classmethod return types
- The domain/models directory is gitignored - use `git add -f` to add new model files
- Chunking with overlap means total_words sum includes overlaps (2000 words + overlap = 2200 total_words)
- TextChunker._WORD_PATTERN.findall() is the reliable way to count words (handles irregular spacing)
- For paragraph chunking, exclude delimiter length when calculating text position: end_pos - len(match.group())

---

## Next Story for Ralph
- **BRAIN-007**: Integration: RAG-Enhanced Generation
- Priority: 7
- Focus: Update LLMWorldGenerator to inject retrieved context into prompts
- Dependencies: BRAIN-006 (Context Retrieval Service)
- Dependencies: BRAIN-004 (KnowledgeIngestionService)

---
- Implemented KnowledgeIngestionService with full ingestion pipeline (Text -> Chunk -> Embed -> Store)
- Created `ingest()` method for single entry ingestion with configurable chunking strategy
- Created `batch_ingest()` method for bulk processing with progress tracking via IngestionProgress callback
- Created `delete()` method to remove all chunks for a source with optional source_type filter
- Created `update()` method to replace old chunks with new content (delete + ingest)
- Created `query_by_source()` method to retrieve all chunks for a given source
- Added health_check() and get_count() utility methods
- Implemented value objects: IngestionProgress, IngestionResult, BatchIngestionResult, SourceChunk, RetrievedChunk
- Files changed:
  - `src/contexts/knowledge/application/services/knowledge_ingestion_service.py`: Complete service implementation
  - `src/contexts/knowledge/application/services/__init__.py`: Service module exports
  - `tests/unit/contexts/knowledge/application/services/test_knowledge_ingestion_service.py`: 23 comprehensive tests

**Learnings for future iterations:**
- Use `embed_batch()` for efficiency instead of calling `embed()` in a loop
- The service layer should use `TYPE_CHECKING` for imports that are only needed for type hints
- Progress callbacks can be sync or async - use `inspect.iscoroutinefunction()` to detect
- Mock vector stores need to track state properly for delete operations to be testable
- ChunkingStrategy.from_string() normalizes string inputs to SourceType enum
- VectorDocument metadata is the place to store source tracking info (source_id, source_type, chunk_index)

---

## [2025-02-03] - BRAIN-005 ✅
- Implemented KnowledgeSyncEventHandler with async queue processing for knowledge ingestion
- Created KnowledgeEventSubscriber to bridge domain events (CharacterCreated/Updated, LoreCreated/Updated, SceneCreated/Updated) to ingestion
- Implemented async queue with configurable max_queue_size to avoid blocking
- Added retry logic with exponential backoff (configurable: FIXED/EXPONENTIAL/NONE)
- Implemented dead letter queue for failed tasks with manual retry capability
- Added content formatting helpers: _character_to_content, _lore_to_content, _scene_to_content
- Created IngestionTask value object for tracking retry state
- All 36 unit tests passing (14 for subscriber, 22 for handler)
- Files changed:
  - `src/contexts/knowledge/application/event_handlers/knowledge_sync_event_handler.py`: Main handler with queue, retry, dead letter
  - `src/contexts/knowledge/application/event_handlers/knowledge_event_subscriber.py`: Event subscriber bridging events to ingestion
  - `src/contexts/knowledge/application/event_handlers/__init__.py`: Package exports
  - `tests/unit/contexts/knowledge/application/event_handlers/test_knowledge_sync_event_handler.py`: 22 handler tests
  - `tests/unit/contexts/knowledge/application/event_handlers/test_knowledge_event_subscriber.py`: 14 subscriber tests

**Learnings for future iterations:**
- Async queue processing requires proper event loop management - asyncio.create_task() must be tracked for cleanup
- Use `asyncio.create_task()` with callback to track pending tasks: `task.add_done_callback(lambda t: self._pending_retry_tasks.discard(t))`
- On handler stop, gather all pending retry tasks with `asyncio.gather(*tasks, return_exceptions=True)` for clean shutdown
- asyncio.Queue(0) doesn't work as expected for testing full queue - it's a special synchronous queue
- For testing async handlers with queues, use short delays and queue.join() to wait for processing
- Test dead letter queue by setting max_retries=0 to skip the retry delay
- Content formatting helpers should include all relevant metadata (IDs, categories, tags) for RAG context

---

## [2025-02-04] - BRAIN-006 ✅
- Implemented RetrievalService with full RAG retrieval pipeline (Query -> Embed -> Search -> Filter -> Format)
- Created `retrieve_relevant()` method with configurable k, filters, and options
- Created `format_context()` method that converts chunks to LLM-ready context with token estimation
- Implemented RetrievalFilter for source_type, tags, and date range filtering
- Added RetrievalOptions for relevance threshold (min_score) and deduplication control
- Implemented content-based deduplication using SequenceMatcher for similarity detection
- Created value objects: FormattedContext, RetrievalResult, RetrievalFilter, RetrievalOptions
- All 36 unit tests passing
- Files changed:
  - `src/contexts/knowledge/application/services/retrieval_service.py`: Complete RetrievalService implementation
  - `src/contexts/knowledge/application/services/__init__.py`: Export RetrievalService and related types
  - `tests/unit/contexts/knowledge/application/services/test_retrieval_service.py`: 36 comprehensive tests

**Learnings for future iterations:**
- For token estimation, use ~4 chars per token as a rough estimate (actual tokenization varies by model)
- When deduplicating, sort by score first so highest-scoring chunks are kept when duplicates are found
- Use `set()` for seen_hashes, not `[]` - type hint `set[str] = []` creates a list, not a set
- The `matches()` method should distinguish between `None` metadata (no document) and `{}` metadata (document with no fields)
- ChromaDB's `$in` operator works for OR queries on metadata fields (e.g., source_type in [CHARACTER, LORE])
- SequenceMatcher from difflib provides decent similarity detection for text deduplication
- When fetching k results, request 2x-3x from vector store to account for filtering/deduplication

---

## [2025-02-04] - BRAIN-007A ✅
- Implemented RAGIntegrationService as a high-level wrapper for RAG prompt enrichment
- Created `enrich_prompt(query, base_prompt)` method that retrieves, formats, and injects context
- Added RAGConfig value object for max_chunks, score_threshold, context_token_limit, enabled flags
- Implemented RAGMetrics for tracking queries_total, chunks_retrieved_total, tokens_added_total, failed_queries
- Added factory method `create()` for convenient dependency injection
- Support for RetrievalFilter and per-call config_override
- All 25 unit tests passing
- Files changed:
  - `src/contexts/knowledge/application/services/rag_integration_service.py`: Complete RAGIntegrationService implementation
  - `src/contexts/knowledge/application/services/__init__.py`: Export new service and types
  - `tests/unit/contexts/knowledge/application/services/test_rag_integration_service.py`: 25 comprehensive tests

**Learnings for future iterations:**
- RAGIntegrationService provides a cleaner abstraction than directly using RetrievalService for prompt enrichment
- The service follows a clear pattern: Retrieve -> Format -> Inject -> Return EnrichedPrompt
- Metrics tracking is valuable for debugging RAG performance (avg chunks per query, tokens added)
- Config overrides allow per-request customization without changing global configuration
- When RAG is disabled (enabled=False), the service returns base_prompt unchanged - useful for A/B testing
- The `@dataclass(frozen=True, slots=True)` pattern is ideal for immutable config value objects
- Use `TYPE_CHECKING` for imports only needed in type hints to avoid circular dependencies

---

## [2025-02-04] - BRAIN-007B ✅
- Added optional RAGIntegrationService dependency to LLMWorldGenerator
- Modified `generate_dialogue()` to async with RAG context enrichment before LLM call
- Modified `suggest_next_beats()` to async with RAG context enrichment before LLM call
- Added `_enrich_with_rag()` helper method that calls RAGIntegrationService.enrich_prompt()
- Added `_extract_keywords_for_dialogue()` for building RAG queries from character data
- Added `_extract_keywords_for_beats()` for building RAG queries from beat context
- Added `use_rag` parameter to enable/disable RAG per request (default: True)
- Updated 3 existing tests to async (suggest_next_beats integration tests)
- All 75 unit tests passing (14 new RAG integration tests)
- Files changed:
  - `src/contexts/world/infrastructure/generators/llm_world_generator.py`: Added RAG integration to dialogue and beat suggestion
  - `tests/unit/contexts/world/infrastructure/test_llm_world_generator.py`: Added 14 RAG integration tests

**Learnings for future iterations:**
- When changing a method from sync to async, all calling tests must also be marked async with `@pytest.mark.asyncio`
- Use `TYPE_CHECKING` for type hints of optional dependencies (RAGIntegrationService) to avoid import errors
- The `use_rag` parameter allows per-request control without changing the generator configuration
- Keyword extraction should include character name, top traits, context keywords, and mood direction
- RAG enrichment gracefully falls back to original prompt on error - logged as warning level
- The generator must remain backward compatible (works without RAG service configured)
- When using `await` in tests, ensure the test function is marked with `@pytest.mark.asyncio`

---

## [2025-02-04] - BRAIN-007C ✅
- Updated `_enrich_with_rag()` to return tuple `(prompt, chunks_retrieved, tokens_added)` instead of just prompt
- Modified `generate_dialogue()` to inject RAG context into **system prompt** (not user prompt)
- Modified `suggest_next_beats()` to inject RAG context into **system prompt** (not user prompt)
- Enhanced logging from `debug` to `info` level for RAG enrichment metrics
- Added `rag_context_injected` info log after successful RAG enrichment
- Added 2 new tests: `test_rag_context_injected_into_system_prompt` and `test_rag_context_injection_logs_metrics`
- All 82 unit tests passing (up from 75 - added 7 new tests for BRAIN-007C)
- Files changed:
  - `src/contexts/world/infrastructure/generators/llm_world_generator.py`: Updated RAG context injection to system prompt
  - `tests/unit/contexts/world/infrastructure/test_llm_world_generator.py`: Updated existing tests and added 7 new tests

**Learnings for future iterations:**
- The System Prompt is the correct place to inject RAG context because it provides consistent background knowledge
- The User Prompt should remain clean and focused on the specific request/task
- Returning metrics (chunks, tokens) from `_enrich_with_rag()` enables better observability without additional log calls
- When updating return types of methods, all test assertions must be updated to unpack the new return values
- The `use_rag` parameter already existed from BRAIN-007B - this story focused on proper context placement
- Context injection follows the pattern: "Relevant Context:\n{context}\n\n---\n\n{original_prompt}"
- RAGIntegrationService's `enrich_prompt()` method already handles the "Relevant Context:" header formatting
- Tests that verify internal behavior (like which prompt receives context) require mocking `_call_gemini` to inspect arguments

---

## [2025-02-04] - BRAIN-008A ✅
- Implemented BM25Retriever service for keyword-based search using BM25Plus algorithm
- Added `index_documents()` method for indexing documents with configurable k1 and b parameters
- Added `search(query, k, collection, filters)` method returning BM25Result list sorted by relevance
- Added `remove_document()` and `clear_collection()` methods for index management
- Added `get_stats()` method returning BM25IndexStats with document count, token count, and average doc length
- Implemented `tokenize()` utility function for word-based tokenization with lowercase normalization
- Created value objects: IndexedDocument (frozen), BM25Result (mutable), BM25IndexStats (frozen)
- Added rank-bm25>=0.2.2 dependency to pyproject.toml
- All 51 unit tests passing (including tokenization, indexing, search, filters, and statistics)
- Files changed:
  - `pyproject.toml`: Added rank-bm25>=0.2.2 dependency
  - `src/contexts/knowledge/application/services/bm25_retriever.py`: Complete BM25Retriever implementation (575 lines)
  - `src/contexts/knowledge/application/services/__init__.py`: Export new BM25 types
  - `tests/unit/contexts/knowledge/application/services/test_bm25_retriever.py`: 51 comprehensive tests

**Learnings for future iterations:**
- BM25Plus works better than BM25Okapi for small corpora because it adds a delta parameter that prevents zero IDF scores
- With BM25, when a term appears in exactly half the documents (df = N/2), IDF becomes log(1) = 0, resulting in zero scores
- BM25 scores can be negative for short documents or rare terms - this is normal behavior due to length normalization
- The `k1` parameter controls term saturation (1.0-2.0 typical): higher = more weight to term frequency
- The `b` parameter controls length normalization (0.0-1.0): higher = more penalty for longer documents
- For keyword search, tokens should be pre-computed and stored to avoid re-tokenizing on every search
- Use `type: ignore[import-untyped]` for libraries without type stubs like rank-bm25
- When using argsort on numpy arrays, reverse with `[::-1]` to get highest scores first

---

## [2025-02-04] - BRAIN-008B ✅
- Implemented HybridRetriever service combining vector (semantic) and BM25 (keyword) search
- Created HybridConfig value object with configurable weights and RRF parameters
- Implemented Reciprocal Rank Fusion (RRF) with configurable k constant (default 60)
- Implemented Linear Score Fusion for pure score-based combination
- Implemented Hybrid Score Fusion combining RRF and linear with alpha parameter (0.0-1.0)
- Added support for per-search config_override to customize fusion behavior
- Graceful degradation: if one search method fails, returns results from the other
- Content-based deduplication using MD5 hashing
- All 44 unit tests passing (covering normalization, RRF, linear fusion, hybrid fusion, config, retriever)
- Files changed:
  - `src/contexts/knowledge/application/services/hybrid_retriever.py`: Complete HybridRetriever implementation (730 lines)
  - `src/contexts/knowledge/application/services/__init__.py`: Export new HybridRetriever types
  - `tests/unit/contexts/knowledge/application/services/test_hybrid_retriever.py`: 44 comprehensive tests

**Learnings for future iterations:**
- Hybrid retrieval provides the best of both worlds: semantic understanding from vector search + exact keyword matching from BM25
- Reciprocal Rank Fusion (RRF) is robust to score scale differences because it operates on ranks, not raw scores
- The RRF k parameter controls the influence of rank (default 60): smaller k = top ranks dominate, larger k = more influence from lower ranks
- Linear fusion requires careful normalization because vector scores (0-1) and BM25 scores (unbounded) have different scales
- The rrf_alpha parameter allows blending between pure rank-based (1.0) and pure score-based (0.0) fusion
- When using TYPE_CHECKING for imports, make sure to use correct relative import depth (e.g., `...domain.models` not `.domain.models`)
- For frozen dataclasses, `__post_init__` must have `-> None` return type annotation to satisfy mypy
- The fusion method naming: "linear" (use_rrf=False or rrf_alpha=0), "rrf" (rrf_alpha=1.0), "hybrid" (0 < rrf_alpha < 1.0)
- Deduplication should use content hash (MD5) to detect identical chunks from both sources
- When ranking results from different sources, store both score and rank for fusion algorithms

---

## [2025-02-04] - BRAIN-009A ✅
- Implemented ILLMClient port interface for LLM abstraction
- Implemented GeminiLLMClient adapter with Gemini API integration
- Implemented QueryRewriter service with three strategies:
  * SYNONYM: Expand queries with synonyms and related terms
  * DECOMPOSE: Break complex queries into sub-queries
  * HYBRID: Combine both approaches
- Added caching for rewritten queries to reduce token usage
- Added graceful degradation on LLM errors (returns original query)
- Added MockLLMClient for testing without API calls
- All 49 unit tests passing
- Files changed:
  - `src/contexts/knowledge/application/ports/i_llm_client.py`: ILLMClient port, LLMRequest, LLMResponse value objects
  - `src/contexts/knowledge/infrastructure/adapters/gemini_llm_client.py`: GeminiLLMClient and MockLLMClient
  - `src/contexts/knowledge/application/services/query_rewriter.py`: Complete QueryRewriter implementation (550 lines)
  - `src/contexts/knowledge/application/services/__init__.py`: Export new types
  - `tests/unit/contexts/knowledge/application/services/test_query_rewriter.py`: 49 comprehensive tests

**Learnings for future iterations:**
- The ILLMClient port interface provides abstraction for different LLM providers (OpenAI, Anthropic, Gemini, etc.)
- When creating async services, always mark test methods with @pytest.mark.asyncio
- RewriteStrategy as str, Enum works for string-compatible enums (strategy.value == "synonym")
- Query rewriting uses lower temperature (0.3) for more deterministic rewrites
- Cache keys should be strategy-specific and case-insensitive for better hit rates
- Graceful degradation: on LLM error, return original query rather than failing
- JSON parsing from LLM responses needs multiple fallback strategies (direct parse, markdown extraction, line splitting)

---

## [2025-02-04] - BRAIN-009B ✅
- Enhanced QueryRewriter with CLARIFICATION strategy for ambiguous queries
- Added token usage tracking to caching (tokens_saved, tokens_used fields)
- Created QueryAwareRetrievalService that integrates QueryRewriter with RetrievalService
- Implemented multi-query execution with concurrency control (Semaphore)
- Implemented Reciprocal Rank Fusion (RRF) and score-based result merging
- Added QueryAwareMetrics for tracking queries, cache hits, and token usage
- All 70 unit tests passing (49 for QueryRewriter, 21 for QueryAwareRetrievalService)
- Files changed:
  - `src/contexts/knowledge/application/services/query_rewriter.py`: Added CLARIFICATION strategy, token tracking, clarifications field
  - `src/contexts/knowledge/application/services/query_aware_retrieval_service.py`: Complete new service (550 lines)
  - `src/contexts/knowledge/application/services/__init__.py`: Export new service and types
  - `tests/unit/contexts/knowledge/application/services/test_query_aware_retrieval_service.py`: 21 comprehensive tests

**Learnings for future iterations:**
- The CLARIFICATION strategy generates questions to disambiguate user queries (e.g., "Did you mean the protagonist or antagonist?")
- Token estimation uses ~4 chars per token as a rough estimate (sufficient for cache efficiency tracking)
- When using frozen dataclass with tuple default values, use `tuple[str, ...]` as type annotation
- Reciprocal Rank Fusion (RRF) formula: score = 1 / (k + rank) where k is typically 60
- RRF is robust to score scale differences because it operates on ranks, not raw scores
- Semaphore limits concurrent retrieval queries to avoid overwhelming the system
- Content deduplication after merging uses MD5 hash to detect identical chunks
- The QueryAwareRetrievalService provides a complete RAG pipeline: Rewrite -> Retrieve (all variants) -> Merge -> Deduplicate

---

## [2025-02-04] - BRAIN-010A ✅
- Created IReranker port interface with rerank(query, results, top_k) method
- Created RerankResult value object with index, score, and relevance_score fields
- Created RerankOutput value object with results, query, total_reranked, model, and latency_ms
- Implemented RerankService with graceful fallback to original order on failure
- Added RerankConfig for enabled/disabled state, fallback behavior, and min_score_threshold
- Created MockReranker for testing without external APIs
- Created FailingReranker for testing fallback behavior
- All 26 unit tests passing (19 for RerankService, 4 for IReranker port, 3 for value objects)
- Files changed:
  - `src/contexts/knowledge/application/ports/i_reranker.py`: IReranker port, RerankResult, RerankOutput, RerankerError
  - `src/contexts/knowledge/application/services/rerank_service.py`: Complete RerankService with MockReranker and FailingReranker
  - `src/contexts/knowledge/application/services/__init__.py`: Export new reranker types
  - `tests/unit/contexts/knowledge/application/services/test_rerank_service.py`: 19 comprehensive tests
  - `tests/unit/contexts/knowledge/application/ports/test_i_reranker.py`: 7 port and value object tests

**Learnings for future iterations:**
- The IReranker port provides abstraction for different reranking providers (Cohere, local sentence-transformers, etc.)
- Frozen dataclasses raise `FrozenInstanceError` not `TypeError` when attempting to modify attributes
- RerankerError should be a distinct exception type from generic Exception for proper error handling
- Fallback to original order is critical for graceful degradation when reranking fails
- The `top_k` parameter limits output while preserving the ability to process more results for filtering
- Mock rerankers are essential for testing services without external API dependencies
- Latency tracking (latency_ms) helps identify performance bottlenecks in the retrieval pipeline
- Minimum score threshold filtering happens after reranking, not before

---

## [2025-02-04] - BRAIN-010B ✅
- Implemented CohereReranker adapter using Cohere Rerank API (rerank-v3.5)
- Implemented LocalReranker adapter using sentence-transformers cross-encoder models
- Added RerankDocument value object to pass content to rerankers (was missing from BRAIN-010A)
- Updated IReranker protocol to accept RerankDocument instead of just RerankResult
- Added score_improvement tracking to RerankOutput for measuring reranking effectiveness
- Added NoOpReranker for testing and graceful fallback
- Implemented create_reranker() factory function for configurable reranker selection
- Added cohere>=5.0.0 and sentence-transformers>=2.2.0 dependencies to pyproject.toml
- All 55 unit tests passing (19 for RerankService, 29 for reranker adapters, 7 for port)
- Files changed:
  - `pyproject.toml`: Added cohere>=5.0.0,<6.0.0 and sentence-transformers>=2.2.0,<4.0.0 dependencies
  - `src/contexts/knowledge/application/ports/i_reranker.py`: Added RerankDocument VO, updated IReranker to accept documents, added score_improvement to RerankOutput
  - `src/contexts/knowledge/application/services/rerank_service.py`: Updated to pass RerankDocument with content, added score_improvement to RerankServiceResult, added RerankerType enum and create_reranker factory
  - `src/contexts/knowledge/infrastructure/adapters/reranker_adapters.py`: Complete implementations of CohereReranker, LocalReranker, NoOpReranker (550+ lines)
  - `tests/unit/contexts/knowledge/application/services/test_rerank_service.py`: Updated tests for new interface
  - `tests/unit/contexts/knowledge/infrastructure/adapters/test_reranker_adapters.py`: 29 comprehensive tests for new adapters

**Learnings for future iterations:**
- Rerankers need access to document content, not just scores - RerankDocument includes index, content, and score
- Cohere's rerank API returns `index` and `relevance_score` in results array - need to map back to original documents
- sentence-transformers CrossEncoder.predict() returns raw scores that need normalization to 0-1 range
- The factory pattern with RerankerType enum allows easy switching between rerankers (cohere/local/mock/noop)
- Lazy loading of sentence-transformers models avoids import errors when package isn't installed
- Score improvement tracking: avg_new_score - avg_original_score, useful for measuring reranking effectiveness
- httpx.AsyncClient requires proper context manager usage for connection pooling
- Early return for empty documents saves unnecessary API calls and edge case handling
- For type hints, use `str` instead of `str | None` when a value is guaranteed to be non-null after initialization

## [2025-02-04] - BRAIN-011A ✅
- Implemented TokenCounter utility class with multi-provider support
- Created LLMProvider enum (OPENAI, ANTHROPIC, GEMINI, COHERE, GENERIC)
- Created ModelFamily enum for different tokenizer families within providers
- Added `count(text, model, provider, model_family)` method with TokenCountResult
- Added `count_batch(texts)` for efficient batch token counting
- Added `count_from_messages(messages)` for chat format token counting (includes overhead)
- Added `estimate_max_chunks(text, max_tokens)` to estimate how many chunks fit in budget
- Added `truncate_to_tokens(text, max_tokens, add_ellipsis)` to truncate text to token limit
- Added `is_available()` method to check tiktoken availability
- Implemented auto-detection of provider/family from model names (GPT-4o, Claude-3, Gemini, etc.)
- Added factory function `create_token_counter(model, provider)` for easy instantiation
- All 49 unit tests passing (2 skipped due to tiktoken being available)
- Files changed:
  - `src/contexts/knowledge/application/services/token_counter.py`: Complete TokenCounter implementation (500+ lines)
  - `src/contexts/knowledge/application/services/__init__.py`: Export new TokenCounter types
  - `tests/unit/contexts/knowledge/application/services/test_token_counter.py`: 51 comprehensive tests

**Learnings for future iterations:**
- TokenCounter uses `str` enum values for compatibility (LLMProvider(str, Enum) pattern)
- Model family detection uses prefix matching for flexible model name variants (e.g., "gpt-4o-mini" matches "gpt-4o")
- tiktoken encoding names differ by model family: "o200k_base" for GPT-4o, "cl100k_base" for GPT-4/GPT-3.5
- Anthropic uses tiktoken's cl100k_base as a close approximation (no official tiktoken support)
- Gemini and Cohere fall back to character-based estimation (~4 chars per token)
- Chat message format overhead is ~4 tokens per message + 3 tokens for reply priming
- For frozen dataclasses with Literal types, use `# type: ignore[arg-type]` when the value is guaranteed valid
- The `TIKTOKEN_AVAILABLE` flag allows graceful degradation when tiktoken isn't installed
- Caching tiktoken encodings by ModelFamily avoids repeated `tiktoken.get_encoding()` calls

---

## [2025-02-04] - BRAIN-011B ✅
- Implemented ContextOptimizer service with 4 packing strategies
- Created `RelevancePackingStrategy` - prioritizes chunks by relevance score only
- Created `DiversityPackingStrategy` - maximizes source diversity via round-robin selection
- Created `RemoveRedundancyPackingStrategy` - removes similar content using SequenceMatcher
- Created `CompressSummariesPackingStrategy` - compresses low-relevance chunks to summaries
- Added `optimize_context(chunks, max_tokens, strategy, config)` method with full configuration support
- Added `calculate_available_tokens()` to reserve space for system prompt and response
- Added `estimate_chunk_tokens()` for batch token estimation
- Created value objects: OptimizationConfig, OptimizationResult, ChunkPriority, OptimizedChunk
- All 32 unit tests passing
- Files changed:
  - `src/contexts/knowledge/application/services/context_optimizer.py`: Complete ContextOptimizer implementation (600+ lines)
  - `src/contexts/knowledge/application/services/__init__.py`: Export new ContextOptimizer types
  - `tests/unit/contexts/knowledge/application/services/test_context_optimizer.py`: 32 comprehensive tests

**Learnings for future iterations:**
- Strategy pattern via IPackingStrategy ABC allows easy extension with new packing algorithms
- Round-robin selection for diversity requires tracking indices per source
- SequenceMatcher from difflib provides decent content similarity for redundancy detection
- Summary generation uses first/last sentence extraction - simple but effective for low-relevance chunks
- Token budget must account for: system_prompt_tokens + overhead_tokens + reserve_for_response
- The `optimize_context()` method returns OptimizationResult with metrics (tokens_saved, compressed, removed_redundant)
- Factory function `create_context_optimizer()` provides convenient instantiation with defaults
- When implementing async strategies, all test methods must use `@pytest.mark.asyncio`

---

## [2025-02-04] - BRAIN-012 ✅
- Implemented CitationFormatter service with multiple citation styles (numeric, alphabetic, source_type)
- Created SourceReference, ChunkCitation, and CitationFormat value objects
- Added get_sources() method to RetrievalService for extracting unique sources from chunks
- Extended FormattedContext with source_references dict for detailed citation data
- Added _get_source_type_prefix() method for citation ID generation (C, L, S, P, I, Loc)
- Created _format_chunk_with_citation() helper for chunk formatting with citations
- All 33 new tests passing (20 for CitationFormatter, 13 for RetrievalService citation features)
- Files changed:
  - `src/contexts/knowledge/application/services/citation_formatter.py`: New CitationFormatter service (450+ lines)
  - `src/contexts/knowledge/application/services/retrieval_service.py`: Added get_sources, _get_source_type_prefix, _format_chunk_with_citation, extended format_context with citation data
  - `src/contexts/knowledge/application/services/__init__.py`: Export citation types (CitationFormatter, SourceReference, ChunkCitation, CitationFormat, CitationFormatterConfig)
  - `tests/unit/contexts/knowledge/application/services/test_citation_formatter.py`: 20 comprehensive tests
  - `tests/unit/contexts/knowledge/application/services/test_retrieval_service.py`: Added 13 tests for get_sources and citation formatting

**Learnings for future iterations:**
- Citation formatting separates concerns: marker generation ([1], source lists, and reference dictionaries
- Source type prefixes help users quickly identify citation types: C (Character), L (Lore), S (Scene), P (Plotline), I (Item), Loc (Location)
- The `get_sources()` method groups chunks by (source_type, source_id) and calculates average relevance scores
- When using frozen dataclasses with tuple default values, use `tuple[str, ...]` as type annotation
- Alphabetic citation IDs use base-26 style: 1=a, 26=z, 27=aa, 52=az, 53=ba for flexible numbering
- CitationFormatterConfig allows customization: format_style, include_chunk_index, include_relevance, max_sources_display
- The `include_citation_data` parameter in format_context() adds structured source data without changing text output
- Test classes outside the main test class need their own fixture definitions

---

## [2025-02-04] - BRAIN-013A ✅
- Implemented QueryDecomposer service to break complex queries into sub-queries
- Created MultiHopRetriever service for chained reasoning across retrieval hops
- Added HopConfig, MultiHopConfig, HopResult, and MultiHopResult value objects
- Implemented LLM-based query decomposition with SYNONYM/DECOMPOSE strategies
- Added max_hops limit (default 3) to prevent infinite loops
- Implemented reasoning chain tracking for debugging and explainability
- Added answer synthesis from all retrieved chunks using LLM
- Implemented graceful degradation on LLM errors (returns original query)
- Implemented early termination when sufficient chunks found or hop returns empty
- All 47 unit tests passing
- Files changed:
  - `src/contexts/knowledge/application/services/multi_hop_retriever.py`: Complete implementation (850+ lines)
  - `src/contexts/knowledge/application/services/__init__.py`: Export new MultiHopRetriever types
  - `tests/unit/contexts/knowledge/application/services/test_multi_hop_retriever.py`: 47 comprehensive tests

**Learnings for future iterations:**
- Multi-hop retrieval enables complex question answering by chaining multiple retrieval steps
- The decompose method uses LLM to break queries into sub-queries (e.g., "Who killed the king?" -> ["Who killed the king?", "What was their motive?"])
- Early termination conditions: enough chunks (>=10) or last hop returned 0 chunks
- The reasoning_chain property provides debug visibility into how each hop informed the next
- Hop tracking includes: hop_number, query, chunks, status, latency_ms, and context_used
- Graceful degradation: on LLM error, return original query as single sub-query
- JSON parsing from LLM responses needs multiple fallback strategies (direct parse, markdown extraction, line splitting)
- The `time` module must be imported at the top of the file for use across all methods
- When using `patch.object` to mock methods in tests, ensure the method signature matches
- The `_needs_decomposition` method uses simple heuristics (conjunctions, multiple question marks) to detect complex queries

---

## [2025-02-04] - BRAIN-013B ✅
- Implemented reasoning chain logging with detailed step tracking for multi-hop retrieval
- Added ExplainConfig value object for explain mode configuration (enabled, include_chunk_content, include_source_info, max_content_length)
- Added ReasoningStep value object with detailed hop information:
  * hop_number, query, query_type (original/decomposed/followup)
  * chunks_found, top_sources (tuple of source identifiers), latency_ms, context_summary
- Extended MultiHopConfig with explain field for explain mode
- Extended MultiHopResult with reasoning_steps list and get_explain_output(verbose) method
- Extended HopResult with reasoning_step field containing detailed ReasoningStep
- Added _extract_top_sources() helper to extract top 3 source identifiers from chunks
- Added _build_reasoning_chain() helper to build human-readable reasoning chain format
- Enhanced logging in retrieval loop with reasoning_step details at info level
- Added debug logging for full reasoning chain after retrieval completes
- All 58 unit tests passing (47 original + 11 new for BRAIN-013B)
- Files changed:
  - `src/contexts/knowledge/application/services/multi_hop_retriever.py`: Added ~150 lines (ExplainConfig, ReasoningStep, enhanced logging, helper methods)
  - `src/contexts/knowledge/application/services/__init__.py`: Export ExplainConfig and ReasoningStep
  - `tests/unit/contexts/knowledge/application/services/test_multi_hop_retriever.py`: Added 11 tests for reasoning chain features

**Learnings for future iterations:**
- The new reasoning chain format uses "Step" instead of "Hop" for clearer language
- ReasoningStep.to_explain_line() provides a human-readable one-line format for each step
- get_explain_output() generates a formatted multi-line explanation with all key details
- Query type tracking (original/decomposed/followup) helps understand how each query was generated
- Top sources extraction uses "SourceType:source_id" format for clear identification
- The reasoning_steps list provides structured access to hop details for programmatic use
- ReasoningStep is a frozen dataclass to ensure immutability of the reasoning history
- Context summaries are truncated to 100 chars to avoid cluttering logs
- The explain mode is disabled by default (enabled=False) to avoid performance impact
- When updating reasoning chain format, update tests that check for specific text patterns

---

## [2025-02-04] - BRAIN-014A ✅
- Implemented PromptTemplate entity with version tracking and model configuration
- Created VariableDefinition value object with type validation and coercion
- Created ModelConfig value object for LLM provider configuration
- Implemented IPromptRepository port with CRUD operations, version history, search, and filtering
- Added template validation for undefined variables, syntax errors (unbalanced braces), and required/optional variable checks
- Implemented {{variable}} placeholder syntax with extraction and rendering
- Added factory method `PromptTemplate.create()` for convenient instantiation with auto-detection of variables
- Implemented `create_new_version()` method for version chaining with parent_version_id tracking
- All 68 unit tests passing (54 for PromptTemplate entity, 14 for IPromptRepository port)
- Files changed:
  - `src/contexts/knowledge/domain/models/prompt_template.py`: Complete implementation (670+ lines)
  - `src/contexts/knowledge/domain/models/__init__.py`: Export PromptTemplate, VariableDefinition, VariableType, ModelConfig
  - `src/contexts/knowledge/application/ports/i_prompt_repository.py`: Repository port interface (170+ lines)
  - `tests/unit/contexts/knowledge/domain/models/test_prompt_template.py`: 54 comprehensive tests
  - `tests/unit/contexts/knowledge/application/ports/test_i_prompt_repository.py`: 14 port tests with mock implementation

**Learnings for future iterations:**
- The PromptTemplate entity follows the existing pattern: dataclass with frozen value objects (VariableDefinition, ModelConfig) and mutable entity (PromptTemplate)
- VariableDefinition.validate_value() and coerce_value() provide runtime type safety for template rendering
- The {{variable}} syntax is extracted via regex pattern r"\{\{([a-zA-Z_][a-zA-Z0-9_]*)\}\}" and must match defined variables
- Template validation happens in __post_init__ to catch errors early (undefined variables, empty required fields)
- The IPromptRepository port uses async methods following the existing repository pattern (IKnowledgeRepository)
- ModelConfig encapsulates all LLM parameters (temperature, max_tokens, top_p, penalties, supports_functions)
- Version tracking uses parent_version_id to link versions, with get_version_history() returning the full chain
- The domain/models directory is gitignored - use `git add -f` to add new model files
- Factory methods should handle common defaults (provider, model_name) while allowing full customization
- When using f-strings with braces for error messages, escape them as {{{{var}}}} to produce {{var}}

---
## [2025-02-04] - BRAIN-014B ✅
- Implemented YAMLPromptMigrator to convert hardcoded YAML prompts to PromptTemplate entities
- Created InMemoryPromptRepository adapter implementing IPromptRepository with full CRUD support
- Added support for {{> other_prompt}} include syntax with circular reference detection
- Implemented automatic variable extraction from {{var}} placeholders
- Added configurable model config based on prompt type (dialogue, world, scene, etc.)
- All 61 unit tests passing (31 for repository, 30 for migration)
- Migration script successfully discovers all 7 YAML prompt files in the codebase
- Files changed:
  - `src/contexts/knowledge/infrastructure/adapters/in_memory_prompt_repository.py`: Complete repository adapter (320+ lines)
  - `src/contexts/knowledge/infrastructure/migrations/migrate_yaml_prompts.py`: Migration script (450+ lines)
  - `tests/unit/contexts/knowledge/infrastructure/adapters/test_in_memory_prompt_repository.py`: 31 comprehensive tests
  - `tests/unit/contexts/knowledge/infrastructure/migrations/test_migrate_yaml_prompts.py`: 30 comprehensive tests

**Learnings for future iterations:**
- PromptTemplate.create() doesn't accept model_config directly - use the PromptTemplate constructor instead
- Use `from uuid import uuid4` for generating IDs in migration scripts
- Include pattern regex: `r"\{\{>\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\}\}"` matches {{> prompt_name}}
- Variable pattern regex: `r"\{\{([a-zA-Z_][a-zA-Z0-9_]*)\}\}"` matches {{variable}}
- Circular reference detection is essential when supporting includes - track visited set
- Model configuration defaults should vary by prompt type (dialogue needs higher temp, world gen needs more tokens)
- Tags are auto-generated from context and filename patterns for easy filtering
- The migration script can be run with dry_run=True to preview changes without saving

---

## [2025-02-04] - BRAIN-015 ✅
- Implemented Prompt Management API with full CRUD endpoints
- Created routers/prompts.py with GET/POST/PUT/DELETE endpoints for prompt templates
- GET /api/prompts - List all prompts with tag, model, limit, offset filtering
- POST /api/prompts - Create new prompt template with validation
- GET /api/prompts/{id} - Get specific prompt by ID
- PUT /api/prompts/{id} - Update prompt (creates new version)
- DELETE /api/prompts/{id} - Soft delete prompt
- POST /api/prompts/{id}/render - Render prompt with variables (supports strict mode)
- GET /api/prompts/search - Search prompts by name or description
- GET /api/prompts/tags - Get all unique tags with model-specific grouping
- GET /api/prompts/{id}/versions - Get version history
- GET /api/prompts/health - Health check endpoint (must be before parameterized routes!)
- Created PromptRouterService with to_summary() and to_detail() conversion methods
- All 46 unit tests passing (27 for router, 19 for service)
- Files changed:
  - `src/api/routers/prompts.py`: Complete router implementation (580+ lines)
  - `src/api/services/prompt_router_service.py`: Service layer (330+ lines)
  - `src/api/main_api_server.py`: Registered prompts router with /api prefix
  - `tests/unit/api/routers/test_prompts_router.py`: 27 router tests
  - `tests/unit/api/services/test_prompt_router_service.py`: 19 service tests

**Learnings for future iterations:**
- Route order matters: define specific routes (/health, /search, /tags) BEFORE parameterized routes (/{id})
- The health check endpoint must be defined before /api/prompts/{id} to avoid 'health' being captured as a prompt_id
- When ModelConfig fields are None, use explicit default values (temperature=0.7, max_tokens=1000) instead of passing None
- Optional variables (required=False) must have a default_value to pass validation
- The schema uses `llm_config` as field name with `alias="model_config"` - serialized JSON uses the alias
- Test fixtures using dependency_overrides require importing the router module first
- When updating tests to match API changes, check both the response dict keys and the Pydantic schema aliases
- The service layer uses `llm_config` as the key, but the API response uses `model_config` due to Pydantic alias


## [2025-02-04] - BRAIN-016A ✅
- Implemented PromptVersion entity for explicit version tracking
- Created VersionDiff value object for structured change tracking (content, variables, model_config, metadata)
- Added create_version_diff() method to PromptTemplate for diff generation
- Added helper methods: is_first_version(), is_rollback_target(), get_lineage_description(), add_tag()
- Added from_template() factory method for convenient PromptVersion creation
- All 31 new tests passing (27 for PromptVersion, 4 integration tests)
- Files changed:
  - `src/contexts/knowledge/domain/models/prompt_version.py`: Complete PromptVersion entity (380+ lines)
  - `src/contexts/knowledge/domain/models/prompt_template.py`: Added create_version_diff() method, TYPE_CHECKING imports
  - `src/contexts/knowledge/domain/models/__init__.py`: Export PromptVersion and VersionDiff
  - `tests/unit/contexts/knowledge/domain/models/test_prompt_version.py`: 31 comprehensive tests

**Learnings for future iterations:**
- PromptVersion is a separate entity that tracks version metadata, separate from PromptTemplate itself
- VersionDiff provides structured change tracking with boolean flags for each change type
- The get_lineage_description() method provides human-readable version history (e.g., "v3 (from v2)")
- Rollback targets are identified by tags: "stable", "rollback"
- Version chain uses template_snapshot_id to reference the actual PromptTemplate entity
- parent_version_id points to the previous version's template_snapshot_id for lineage tracking
- When using TYPE_CHECKING for imports, the imported types are only available for type hints, not runtime
- The `isinstance(self.tags, list)` check with tuple type hint causes mypy unreachable warning - use `# type: ignore[unreachable]`

---

## [2025-02-04] - BRAIN-016B ✅
- Implemented version rollback API endpoint POST /api/prompts/{id}/rollback/{version}
- Implemented version comparison API endpoint GET /api/prompts/{id}/compare
- Added rollback_to_version() method to PromptRouterService for restoring previous versions
- Added compare_versions() method with character-level diff using difflib.SequenceMatcher
- Added _compute_character_diff() helper for line-by-line diff computation
- Rollback creates new version (v4) with content from target version (v2)
- Comparison supports content diff, variable changes, model config changes, and metadata changes
- All 185 prompt-related tests passing (26 router + 46 service + 113 domain/model/repository)
- Files changed:
  - `src/api/routers/prompts.py`: Added rollback and compare endpoints (70+ lines)
  - `src/api/services/prompt_router_service.py`: Added rollback_to_version() and compare_versions() methods (280+ lines)
  - `tests/unit/api/services/test_prompt_router_service.py`: Added TestPromptRouterServiceRollback (4 tests) and TestPromptRouterServiceCompare (5 tests)

**Learnings for future iterations:**
- Route order is critical: specific routes (/health, /search, /tags, /versions, /compare) must come BEFORE parameterized routes (/{id})
- When using path parameters in POST endpoint like /rollback/{version}, FastAPI requires the parameter to be after all path segments
- difflib.SequenceMatcher with autojunk=False provides reliable line-by-line diffing for text comparison
- The get_opcodes() method returns operations: "replace" (both old and new), "delete" (old only), "insert" (new only), "equal" (no change)
- When testing router endpoints, use service-level tests rather than complex router tests to avoid dependency override complexity
- Service-level tests are easier to write and maintain than HTTP endpoint tests
- Rollback preserves version history by creating a NEW version (v4) that copies content from target version (v2), not by overwriting

## [2025-02-04] - BRAIN-016B ✅
- Implemented version rollback API endpoint POST /api/prompts/{id}/rollback/{version}
- Implemented version comparison API endpoint GET /api/prompts/{id}/compare
- Added rollback_to_version() method to PromptRouterService for restoring previous versions
- Added compare_versions() method with character-level diff using difflib.SequenceMatcher
- Added _compute_character_diff() helper for line-by-line diff computation
- Rollback creates new version (v4) with content from target version (v2)
- Comparison supports content diff, variable changes, model config changes, and metadata changes
- All 185 prompt-related tests passing (26 router + 46 service + 113 domain/model/repository)
- Files changed:
  - `src/api/routers/prompts.py`: Added rollback and compare endpoints (70+ lines)
  - `src/api/services/prompt_router_service.py`: Added rollback_to_version() and compare_versions() methods (280+ lines)
  - `tests/unit/api/services/test_prompt_router_service.py`: Added TestPromptRouterServiceRollback (4 tests) and TestPromptRouterServiceCompare (5 tests)

**Learnings for future iterations:**
- Route order is critical: specific routes (/health, /search, /tags, /versions, /compare) must come BEFORE parameterized routes (/{id})
- When using path parameters in POST endpoint like /rollback/{version}, FastAPI requires the parameter to be after all path segments
- difflib.SequenceMatcher with autojunk=False provides reliable line-by-line diffing for text comparison
- The get_opcodes() method returns operations: "replace" (both old and new), "delete" (old only), "insert" (new only), "equal" (no change)
- When testing router endpoints, use service-level tests rather than complex router tests to avoid dependency override complexity
- Service-level tests are easier to write and maintain than HTTP endpoint tests
- Rollback preserves version history by creating a NEW version (v4) that copies content from target version (v2), not by overwriting
---

## [2025-02-04] - BRAIN-017A ✅
- Implemented template inheritance for PromptTemplate entity
- Added extends field to PromptTemplate for inheritance declarations
- Added {{> template_name}} include syntax support with regex pattern matching
- Added get_includes() method to extract template names from content
- Added resolve_content() method to replace includes with parent content (recursive with circular reference detection)
- Added resolve_variables() method to merge parent variables with child variables
- Child variables override parent variables with same name
- Added render_with_inheritance() method for rendering with inheritance
- Updated to_dict() and from_dict() to serialize/deserialize extends field
- Updated create_new_version() and create_version_diff() to handle extends
- Updated API schemas: PromptCreateRequest, PromptUpdateRequest, PromptDetailResponse
- Updated PromptRouterService.to_detail() to include extends in API responses
- Updated router create/update endpoints to handle extends field
- All 161 prompt-related tests passing (75 domain + 26 router + 60 service)
- Files changed:
  - `src/contexts/knowledge/domain/models/prompt_template.py`: Added extends field, _INCLUDE_PATTERN, and inheritance methods (200+ lines)
  - `src/api/schemas.py`: Added extends to PromptCreateRequest, PromptUpdateRequest, PromptDetailResponse
  - `src/api/services/prompt_router_service.py`: Updated to_detail() to include extends
  - `src/api/routers/prompts.py`: Updated create and update endpoints to handle extends
  - `tests/unit/contexts/knowledge/domain/models/test_prompt_template.py`: Added TestPromptTemplateInheritance (21 tests)

**Learnings for future iterations:**
- The include pattern regex must support dashes: `[a-zA-Z_][a-zA-Z0-9_-]*` for IDs like `parent-id-123`
- Variables from included templates ({{> parent}}) need to be merged even when extends field is empty
- resolve_variables() should check both extends field AND {{> includes}} in content
- render_with_inheritance() must bypass PromptTemplate validation to avoid errors for resolved content
- Use visited set (copied) for circular reference detection in recursive resolution
- When creating temporary templates for rendering, use the same ID to avoid self-reference detection
- Child variables override parent variables by checking child_names set before adding parent vars

## [2025-02-04] - BRAIN-017B ✅
- Implemented circular dependency detection in extends chain
- Added check_circular_extends() method to PromptTemplate entity
- Detects both direct self-reference and indirect circular chains (A->B->C->A)
- Raises ValueError with helpful error message: "Circular reference detected in extends chain"
- Validates extends chain recursively with visited set tracking
- All 78 prompt template tests passing (up from 75 with 3 new tests for circular detection)
- Files changed:
  - `src/contexts/knowledge/domain/models/prompt_template.py`: Added check_circular_extends() method (30+ lines)
  - `tests/unit/contexts/knowledge/domain/models/test_prompt_template.py`: Added 3 circular detection tests

**Learnings for future iterations:**
- Circular reference detection requires tracking visited set across recursive calls
- Use visited.copy() to avoid mutating the parent's visited set in recursion
- The extends field and {{> includes}} are two different inheritance mechanisms:
  - `extends`: Explicit inheritance declaration for variable merging
  - {{> includes}}: Content composition directives
- Both need circular reference detection but in different contexts
- Self-reference is validated in __post_init__ for extends field, but circular chains need runtime detection

## [2025-02-04] - BRAIN-018A ✅
- Implemented PromptExperiment entity for A/B testing prompt templates
- Created ExperimentStatus enum (DRAFT, RUNNING, PAUSED, COMPLETED, ARCHIVED)
- Created ExperimentMetric enum (SUCCESS_RATE, USER_RATING, TOKEN_EFFICIENCY, LATENCY)
- Created ExperimentMetrics value object with aggregation properties:
  * success_rate: Percentage of successful generations (0-100)
  * avg_rating: Average user rating from feedback
  * avg_tokens_per_run: Average tokens consumed per run
  * token_efficiency: Tokens per successful generation (lower is better)
  * avg_latency_ms: Average response time
- Implemented PromptExperiment entity with full lifecycle:
  * create(): Factory method for convenient instantiation
  * assign_variant(): Consistent hashing using SHA-256 for deterministic assignment
  * record_success()/record_failure(): Track results per variant
  * start()/pause()/resume()/complete(): Lifecycle management
  * _detect_winner(): Auto-detect winner based on configured metric
  * get_results(): Generate experiment results summary with comparison
- Added serialization support (to_dict/from_dict) for persistence
- All 53 unit tests passing
- Files changed:
  - `src/contexts/knowledge/domain/models/prompt_experiment.py`: Complete implementation (650+ lines)
  - `src/contexts/knowledge/domain/models/__init__.py`: Export new types
  - `tests/unit/contexts/knowledge/domain/models/test_prompt_experiment.py`: 53 comprehensive tests

**Learnings for future iterations:**
- Consistent hashing uses SHA-256 on user_id:session_id to ensure same user always gets same variant
- For proper A/B testing, variant assignment must be deterministic - users should not switch between variants
- Hash-based assignment distributes users proportionally to traffic_split (e.g., 30/70)
- ExperimentMetrics validation enforces total_runs == success_count + failure_count for data integrity
- Token efficiency is a key metric - lower tokens per successful generation is better
- The winner detection logic varies by metric type:
  * SUCCESS_RATE: Higher percentage wins
  * USER_RATING: Higher average wins
  * TOKEN_EFFICIENCY: Lower value wins (fewer tokens per success)
  * LATENCY: Lower value wins (faster response)
- When using TYPE_CHECKING for type hints only, remove the import if the type is never actually referenced
- Experiment lifecycle prevents invalid state transitions (e.g., can't start a running experiment)
- min_sample_size ensures statistical significance before declaring winners

## [2025-02-04] - BRAIN-018B ✅
- Implemented Experiment API endpoints for prompt A/B testing
- Created IExperimentRepository port interface following hexagonal architecture
- Created InMemoryExperimentRepository adapter for development/testing
- Implemented ExperimentRouterService with statistical calculations:
  * Two-proportion z-test for comparing success rates
  * Cohen's h effect size measurement
  * Wilson score confidence intervals
  * Normal CDF approximation for p-values
- Created experiments router with 10 REST API endpoints:
  * GET /api/experiments - List with filtering (status, prompt_id, limit, offset)
  * POST /api/experiments - Create A/B test experiment
  * GET /api/experiments/{id} - Get experiment details
  * GET /api/experiments/{id}/results - Get results with statistical analysis
  * POST /api/experiments/{id}/start - Start experiment
  * POST /api/experiments/{id}/pause - Pause experiment
  * POST /api/experiments/{id}/resume - Resume experiment
  * POST /api/experiments/{id}/complete - Complete with optional winner
  * POST /api/experiments/{id}/record - Record generation result
  * DELETE /api/experiments/{id} - Delete experiment
- Added 13 new API schema classes for experiments
- Added ConfidenceIntervalResponse schema for confidence intervals
- Added JSON sanitization for inf/nan values (domain returns inf for token_efficiency with no successes)
- All 27 new tests passing + 76 existing API router tests
- Files changed:
  - `src/api/routers/experiments.py`: Complete router implementation (510+ lines)
  - `src/api/services/experiment_router_service.py`: Service layer with stats (650+ lines)
  - `src/contexts/knowledge/application/ports/i_experiment_repository.py`: Repository port (185 lines)
  - `src/contexts/knowledge/infrastructure/adapters/in_memory_experiment_repository.py`: In-memory adapter (250+ lines)
  - `src/api/schemas.py`: Added 14 new schema classes
  - `src/api/main_api_server.py`: Registered experiments router
  - `tests/unit/api/routers/test_experiments_router.py`: 27 comprehensive tests

**Learnings for future iterations:**
- When domain models return float("inf") for valid business reasons (no successes = infinite cost), sanitize at API layer
- Use `math.isinf(value) or math.isnan(value)` to detect non-finite floats before JSON serialization
- Replace inf with 0.0 (not None) when schema fields are required (e.g., token_efficiency: float)
- Dependency override pattern: import functions directly from router module (`from src.api.routers.experiments import get_experiment_repository`)
- For test fixtures that need multiple repositories, create separate async setup functions and run them with asyncio.run()
- PromptTemplate validation requires VariableDefinition for all variables used in content {{var}}
- Route order matters: /health must be before /{id} routes
- Confidence intervals require special handling in schemas: Optional[ConfidenceIntervalResponse] with lower/upper bounds
- Statistical z-test uses pooled proportion: p_pooled = (p1*n1 + p2*n2) / (n1 + n2)
- Cohen's h for proportions: 2 * (asin(sqrt(p1)) - asin(sqrt(p2)))
- Normal CDF approximation uses Abramowitz and Stegun error function formula
---

## [2025-02-04] - BRAIN-019A ✅
- Implemented Prompt Lab frontend list view with search and filter capabilities
- Added prompt-related Zod schemas to frontend/src/types/schemas.ts
  - PromptVariableDefinitionSchema, PromptVariableValueSchema, PromptModelConfigSchema
  - PromptSummarySchema, PromptDetailResponseSchema, PromptListResponseSchema
  - PromptCreateRequestSchema, PromptUpdateRequestSchema, PromptRenderRequestSchema
  - PromptRenderResponseSchema, PromptTagsResponseSchema
- Created frontend/src/features/prompts/api/promptApi.ts with TanStack Query hooks
  - usePrompts, usePromptsSearch, usePrompt, usePromptVersions, usePromptTags
  - useCreatePrompt, useUpdatePrompt, useDeletePrompt, useRenderPrompt, useRollbackPrompt
- Created PromptsPage component with:
  - Search input for filtering by name/description
  - Model filter dropdown (dynamically populated from available models)
  - Tag filter dropdown (populated from backend tags endpoint)
  - Responsive grid of prompt cards with View/Edit/Delete actions
  - Loading skeleton state and empty state with action button
  - Version badge display, variable count, and last updated date
- Created BrainPromptsPage.tsx route wrapper page
- Added /brain/prompts route to TanStack Router
- Used useNavigate from TanStack Router for navigation
- All type-check and lint passing
- Files changed:
  - `frontend/src/types/schemas.ts`: Added prompt-related Zod schemas
  - `frontend/src/features/prompts/api/promptApi.ts`: API hooks with TanStack Query
  - `frontend/src/features/prompts/components/PromptsPage.tsx`: Main page component
  - `frontend/src/features/prompts/index.ts`: Feature exports
  - `frontend/src/pages/BrainPromptsPage.tsx`: Route wrapper
  - `frontend/src/app/router.tsx`: Added brain/prompts route

**Learnings for future iterations:**
- Use `useNavigate` from '@tanstack/react-router' for type-safe navigation
- EmptyState action prop doesn't accept undefined - conditionally render with/without action
- When building filters object, only include defined properties to satisfy exactOptionalPropertyTypes
- For model_name which can be string | undefined, use `.filter((m): m is string => Boolean(m))` type guard
- Import schemas without Schema suffix for types, and with Schema suffix for Zod schemas
- Route must be added both as a createRoute call AND in the routeTree.addChildren array
- Prompt cards display metadata: version badge, tags (with +N overflow), variable count, model name, date

## [2025-02-04] - BRAIN-019B ✅
- Implemented full-featured Prompt Lab editor with tabbed interface
- Created PromptEditorPage component with Content, Variables, Config, and History tabs
- Added syntax highlighting for {{variable}} placeholders using overlay technique
- Implemented variable definition table with inline editing (name, type, default, required, description)
- Added model configuration selector with provider/model dropdowns and sliders for all LLM parameters
- Added version history sidebar displaying all prompt versions
- Implemented autosave to localStorage with automatic restoration for new prompts
- Added tag management with add/remove functionality
- Added extends (inheritance) management for template composition
- Added "Add detected variables" button to auto-populate variables from {{var}} patterns in content
- All type-check and lint passing
- Files changed:
  - `frontend/src/features/prompts/components/PromptEditorPage.tsx`: New editor component (1035 lines)
  - `frontend/src/pages/BrainPromptEditorPage.tsx`: Route wrapper page
  - `frontend/src/app/router.tsx`: Added /brain/prompts/$id route with parameter
  - `frontend/src/features/prompts/components/PromptsPage.tsx`: Updated navigation to use /brain/prompts/new
  - `frontend/src/features/prompts/index.ts`: Export PromptEditorPage

**Learnings for future iterations:**
- TanStack Router route types need to be cast when using dynamic routes not yet recognized by type system
- The overlay technique for syntax highlighting uses a transparent div with HTML overlay behind textarea
- For overlay highlighting to work, both div and textarea must have identical font, spacing, and padding
- useRef is needed for both textarea and overlay div to sync scroll positions
- usePrompt and usePromptVersions hooks accept only one argument (the id), no config object
- For Partial<> updates to objects with required fields, provide explicit defaults for all required fields
- The variable type enum uses "list" and "dict" not "array" and "json" - must match backend PromptVariableTypeEnum
- Autosave uses localStorage with key prefix pattern: `{PREFIX}{id}` to isolate drafts by prompt ID
- LazyRouteComponent requires proper typing in router.tsx for code splitting to work
- Versions API returns PromptSummary objects (not full PromptDetailResponse) - only has summary fields


## [2025-02-04] - BRAIN-020A ✅
- Implemented PromptPlaygroundModal with split-view playground UI
- Added 'Run' button in PromptEditorPage that opens the playground modal (only visible for existing prompts, not new)
- Created dynamic variable input forms based on prompt variable definitions
- Implemented type-specific input components: text (Textarea), number (Input), boolean (Switch), list/dict (Textarea with JSON)
- Added model configuration UI with provider dropdown, model selector, temperature slider, max_tokens slider
- Added advanced settings toggle for top_p, frequency_penalty, presence_penalty
- Implemented split view layout: rendered prompt on left, LLM output placeholder on right
- Added copy to clipboard buttons for both rendered prompt and LLM output
- Added token count estimation display (~4 chars per token)
- Integrated useRenderPrompt hook for rendering prompts with variables
- Files changed:
  - `frontend/src/features/prompts/components/PromptPlaygroundModal.tsx`: New playground modal component (520+ lines)
  - `frontend/src/features/prompts/components/PromptEditorPage.tsx`: Added Run button in header, playground modal integration
  - `frontend/src/features/prompts/index.ts`: Export PromptPlaygroundModal

**Learnings for future iterations:**
- The Run button should only appear for existing prompts (not "new" unsaved prompts) since we need a prompt_id for rendering
- Variable input types need different UI components: boolean uses Switch, numbers use Input type="number", list/dict use Textarea for JSON input
- Model suggestions array should be indexed by provider name for dynamic model list updates when provider changes
- Token estimation uses ~4 characters per token as a rough approximation (sufficient for UI display)
- Copy feedback uses a 2-second timeout with state tracking to show "Copied" checkmark
- Dialog with max-w-6xl and h-[85vh] provides ample space for the split-view playground while keeping it on screen
- Advanced settings toggle (More/Less) helps declutter the UI while keeping power-user features accessible
- The playground modal should initialize variable values from prompt defaults, with fallback to type-appropriate empty values
- For BRAIN-020B, the LLM output section will need to call a generation endpoint (currently just shows placeholder message)

---
